{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69ba7f",
   "metadata": {
    "id": "0f69ba7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from google.colab import drive\n",
    "!pip install torchmetrics                # ONLY FOR FIRST EXECUTION\n",
    "\n",
    "# --- CONFIGURATIONS: SET PATHS, DEVICE, MODEL AND HYPERPARAMETERS ---\n",
    "\n",
    "PATH_DRIVE = \"/content/drive\"\n",
    "PATH_EXPORT = \"/content/drive/MyDrive/best_model.pth\"\n",
    "PATH_FILE = \"/content/drive/MyDrive/SPair-71k.tar.gz\"\n",
    "\n",
    "PATH_TEST = \"/content/SPair-71k/PairAnnotation/test\"\n",
    "PATH_VAL = \"/content/SPair-71k/PairAnnotation/val\"\n",
    "PATH_TRAIN = \"/content/SPair-71k/PairAnnotation/trn\"\n",
    "\n",
    "ALL_TEST_PATH = \"/content/SPair-71k/Layout/small/test.txt\"\n",
    "ALL_TRAIN_PATH = \"/content/SPair-71k/Layout/small/trn.txt\"\n",
    "ALL_VAL_PATH = \"/content/SPair-71k/Layout/small/val.txt\"\n",
    "\n",
    "IMAGE_FOLDER_NAME = \"/content/SPair-71k/JPEGImages\"\n",
    "PTH_PATH = \"/content/drive/MyDrive/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\"\n",
    "\n",
    "PATH_RES = \"/content/Results\"\n",
    "os.makedirs(PATH_RES, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_VERSION = \"dinov2\"         # \"dinov2\", \"dinov3\", \"sam\"\n",
    "TUNING = False                     # TRAINING OR INFERENCE\n",
    "\n",
    "USE_WIN = True                # SOFTMAX OR ARGMAX\n",
    "WINDOW_SOFTMAX = 7\n",
    "TAU_SOFTMAX = 0.01       # POINT P3 (SIZE: 3,5,7; TAU: 0.01, 0.05, 0.07)\n",
    "\n",
    "IMAGE_SIZE = 1024 if MODEL_VERSION == \"sam\" else 224\n",
    "PATCH_SIZE = 14 if MODEL_VERSION == \"dinov2\" else 16\n",
    "H_PATCH, W_PATCH = IMAGE_SIZE // PATCH_SIZE, IMAGE_SIZE // PATCH_SIZE\n",
    "ALPHA = [0.05, 0.1, 0.2]\n",
    "CATEGORIES = [ \"aeroplane\" ] # , \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"dog\", \"horse\",\n",
    "                #  \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"train\", \"tvmonitor\" ]\n",
    "\n",
    "# --- IF SAM MODEL ---\n",
    "\n",
    "if MODEL_VERSION == \"sam\":\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "    from segment_anything import sam_model_registry, SamPredictor                 # DOWNLOAD\n",
    "\n",
    "    PTH_PATH = \"/content/drive/MyDrive/sam_vit_b_01ec64.pth\"\n",
    "    predictor = None\n",
    "\n",
    "# --- DATASET CLASS ---\n",
    "# --- COLLECT ALL JSON FILE BASED ON CURRENT_CATEGORY OR MODE ---\n",
    "# --- WHEN REQUIRED, OPEN THE NEXT FILE AND TAKE IN A DICTIONARY ALL YOU NEED ---\n",
    "\n",
    "class SPair71kDataset(Dataset):\n",
    "    def __init__(self, pair_path, source_path, category_filter=None):\n",
    "        self.pair_files = []\n",
    "        self.image_path = source_path\n",
    "        file = open(pair_path, \"r\")\n",
    "\n",
    "        for line in file:\n",
    "            (pair_id, category) = line.strip().split(\".json\")[0].split(\":\")\n",
    "            if TUNING or category == category_filter:\n",
    "                self.pair_files.append((line, category))\n",
    "\n",
    "        file.close()\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (json_file, category) = self.pair_files[idx]\n",
    "        file_name = json_file.strip() + \".json\"\n",
    "        json_path = os.path.join(self.image_path, file_name)\n",
    "\n",
    "        file = open(json_path, \"r\")\n",
    "        annotation = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "        src_path = os.path.join(IMAGE_FOLDER_NAME, category, annotation[\"src_imname\"])      # TAKE ALL INFO\n",
    "        trg_path = os.path.join(IMAGE_FOLDER_NAME, category, annotation[\"trg_imname\"])\n",
    "        ids = [int(el) for el in annotation[\"kps_ids\"]]\n",
    "\n",
    "        return {\n",
    "            \"src_path\": src_path,\n",
    "            \"trg_path\": trg_path,\n",
    "            \"src_kps\": np.array(annotation[\"src_kps\"]),\n",
    "            \"trg_kps\": np.array(annotation[\"trg_kps\"]),\n",
    "            \"kps_ids\": np.array(ids),\n",
    "            \"trg_bndbox\": np.array(annotation[\"trg_bndbox\"]),        # DICT of BATCHES (SIZE=1)\n",
    "            \"file_name\": file_name,\n",
    "        }\n",
    "\n",
    "# --- IMAGE PREPROCESSING ---\n",
    "# --- RESIZE IMAGE TO STANDARD MODEL DIMENSIONS, CONVERT IT INTO A TENSOR AND NORMALIZE IT ---\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "# --- UNFREEZE ONLY THE LAST num_last_blocks LAYERS AND THE FINAL LAYER NORM ---\n",
    "# --- FREEZE ALL LAYERS AND THEN FREE THE LASTS (IF ACCESSIBLE) ---\n",
    "\n",
    "def setup_light_finetuning(model, num_last_blocks):\n",
    "    N_Params = 0\n",
    "    N_Free_Params = 0\n",
    "\n",
    "    if MODEL_VERSION == \"sam\":\n",
    "        blocks_to_unfreeze = model.image_encoder.blocks[-num_last_blocks:]\n",
    "        \n",
    "        if hasattr(model.image_encoder, \"post_norm\"):       # NOT SURE THE FINAL NORM IS ACCESSIBLE\n",
    "            norm = model.image_encoder.post_norm\n",
    "        else:\n",
    "            norm = None\n",
    "\n",
    "    else:\n",
    "        blocks_to_unfreeze = model.blocks[-num_last_blocks:]\n",
    "        norm = model.norm\n",
    "\n",
    "    for param in model.parameters():                # FREEZE ALL\n",
    "        N_Params += param.numel()\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for block in blocks_to_unfreeze:\n",
    "\n",
    "        for param in block.parameters():\n",
    "            N_Free_Params += param.numel()           # UNFREEZE\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if norm:\n",
    "        for param in norm.parameters():          # UNFREEZE NORM\n",
    "            N_Free_Params += param.numel()\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # NUMBERS\n",
    "\n",
    "    print(\"Total parameters:\", N_Params)\n",
    "    print(\"Total trainable:\", N_Free_Params)\n",
    "    print(\"Percentage trainable:\", round(100 * N_Free_Params / N_Params, 2), \"%\")\n",
    "    return model\n",
    "\n",
    "# --- LOAD AND PREPROCESS IMAGE. EXTRACT PATCH-WISE FEATURES FROM MODEL. ---\n",
    "# --- RESHAPE AND NORMALIZE FEATURE MAPS. ---\n",
    "# --- FINALLY, RETURN THE EXTRACTED FEATURE MAP AND THE ORIGINAL DIMENSIONS. --\n",
    "\n",
    "def get_descriptors(img_path, grad):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    (w, h) = img.size\n",
    "    input_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.set_grad_enabled(grad):             # ENABLE WEIGTH UPDATING OR NOT\n",
    "\n",
    "        if MODEL_VERSION == \"dinov2\":\n",
    "            feats = model.get_intermediate_layers(input_tensor, n=1)[0]\n",
    "            feats = feats.reshape(1, H_PATCH, W_PATCH, feats.shape[2])                # FOR DINOV2\n",
    "\n",
    "        elif MODEL_VERSION == \"dinov3\":\n",
    "            x = model.forward_features(input_tensor)[\"x_norm_patchtokens\"]            # FOR DINOV3\n",
    "            feats = x.reshape(1, H_PATCH, W_PATCH, x.shape[-1])\n",
    "\n",
    "        elif MODEL_VERSION == \"sam\":\n",
    "\n",
    "            if TUNING:\n",
    "                feats = model.image_encoder(input_tensor)[:, 1:, :]            # FOR TRAINING SAM\n",
    "            else:\n",
    "                predictor.set_image(np.array(img))\n",
    "                feats = predictor.get_image_embedding()                 # FOR INFERENCE WITH SAM\n",
    "                feats = feats.permute(0, 2, 3, 1)\n",
    "\n",
    "    feats = F.normalize(feats, dim=-1)\n",
    "    return (feats, w, h)\n",
    "\n",
    "# --- MATCH KEYPOINTS. ---\n",
    "# --- EXTRACT FEATURE SIZE, CREATE PROBABILITY GRID FOR SOFTMAX ---\n",
    "# --- FOR EACH SRC_KPS RESCALE IT, USE COSINE SIMILARITY METRIC AND COMPUTE THE PREDICTION ---\n",
    "# --- WITH SIMPLE ARGMAX (STEP 1), SIMPLE SOFTMAX (STEP 2) OR WINDOW SOFTMAX (STEP 3) ---\n",
    "# --- IN THE LAST CASE, COMPUTE THE RESULT WITH DIFFERENT WINDOW SIZE AND TAU ---\n",
    "\n",
    "def match_keypoints(batch, grad):\n",
    "    src_kps = batch[\"src_kps\"][0].numpy()\n",
    "\n",
    "    (feat_src, sw, sh) = get_descriptors(batch[\"src_path\"][0], grad)          # GET DESCRIPTORS\n",
    "    (feat_trg, tw, th) = get_descriptors(batch[\"trg_path\"][0] , grad)\n",
    "\n",
    "    (_, Hf, Wf, D) = feat_trg.shape\n",
    "    trg_flat = feat_trg[0].reshape(Hf * Wf, D)\n",
    "    pred_kps = []\n",
    "\n",
    "    for i in range(src_kps.shape[0]):\n",
    "        sx = int(src_kps[i, 0] * Wf / sw)\n",
    "        sy = int(src_kps[i, 1] * Hf / sh)\n",
    "        sx = torch.clamp(torch.tensor(sx, device=DEVICE), 0, Wf - 1)\n",
    "        sy = torch.clamp(torch.tensor(sy, device=DEVICE), 0, Hf - 1)\n",
    "        src_desc = feat_src[0, sy, sx, :]\n",
    "\n",
    "        sim = torch.matmul(trg_flat, src_desc)            # COSINE SIMILARITY\n",
    " \n",
    "        if TUNING:                                                                     # NORMAL SOFTMAX\n",
    "            coords = torch.stack(torch.meshgrid(torch.arange(Wf, device=DEVICE),\n",
    "                                                    torch.arange(Hf, device=DEVICE), indexing='ij'), dim=-1).reshape(-1,2)\n",
    "            prob = F.softmax(sim / TAU_SOFTMAX, dim=0)\n",
    "            pred_xy = (coords.float() * prob[:, None]).sum(dim=0)\n",
    "\n",
    "        elif USE_WIN:\n",
    "            sim_map = sim.reshape(Hf, Wf)\n",
    "\n",
    "            pred_xy = window_soft_argmax(sim_map)              # WINDOW SOFTMAX\n",
    "            (px, py) = (pred_xy[0], pred_xy[1])\n",
    "\n",
    "        else:\n",
    "            best_idx = sim.argmax()\n",
    "            pred_xy = torch.tensor([best_idx % Wf, best_idx // Wf], device=DEVICE)           # NORMAL INFERENCE\n",
    "\n",
    "        pred_x = (pred_xy[0] + 0.5) * (tw / Wf)\n",
    "        pred_y = (pred_xy[1] + 0.5) * (th / Hf)\n",
    "        pred_kps.append(torch.stack([pred_x, pred_y]))\n",
    "    \n",
    "    return torch.stack(pred_kps)\n",
    "\n",
    "# --- USE SOFTARGMAX TO PREDICT THE FINAL VALUE- ---  \n",
    "\n",
    "def window_soft_argmax(similarity_map):\n",
    "    (H, W) = similarity_map.shape\n",
    "\n",
    "    best_idx = similarity_map.argmax()          # NORMAL SIMLARITY\n",
    "    y_peak = best_idx // W\n",
    "    x_peak = best_idx % W\n",
    "\n",
    "    half = WINDOW_SOFTMAX // 2\n",
    "    y0 = max(y_peak - half, 0)\n",
    "    y1 = min(y_peak + half + 1, H)              # WINDOW'S DIMENSIONS\n",
    "    x0 = max(x_peak - half, 0)\n",
    "    x1 = min(x_peak + half + 1, W)\n",
    "\n",
    "    window = similarity_map[y0:y1, x0:x1]           # SOFTMAX WINDOW\n",
    "\n",
    "    (ys, xs) = torch.meshgrid(torch.arange(y0, y1, device=similarity_map.device),\n",
    "                            torch.arange(x0, x1, device=similarity_map.device),\n",
    "                            indexing=\"ij\")\n",
    "    coords = torch.stack([xs.flatten(), ys.flatten()], dim=1).float()\n",
    "\n",
    "    prob = F.softmax(window.flatten() / TAU_SOFTMAX, dim=0)        # NEW SOFTMAX\n",
    "    pred_patch = (coords * prob[:, None]).sum(dim=0)         # FINAL PREDICTION\n",
    "\n",
    "    return pred_patch\n",
    "\n",
    "\n",
    "# --- VISUALIZE RESULTS AND COMPARE CORRECT AND PREDICTED KEYPOINTS ON TARGET IMAGE. ---\n",
    "\n",
    "def visualize_keypoints(src_path, trg_path, src_kps, pred_kps, trg_kps):\n",
    "    src_img = cv2.imread(src_path)[:, :, ::-1]\n",
    "    trg_img = cv2.imread(trg_path)[:, :, ::-1]\n",
    "\n",
    "    (_, axes) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(src_img)\n",
    "    axes[0].scatter(src_kps[:,0], src_kps[:,1], c=\"r\", s=40, label=\"src_kps\")\n",
    "    axes[0].set_title(\"Source Image\")\n",
    "\n",
    "    axes[1].imshow(trg_img)\n",
    "    axes[1].scatter(pred_kps[:,0], pred_kps[:,1], c=\"b\", s=40, label=\"pred_kps\")\n",
    "    axes[1].scatter(trg_kps[:,0], trg_kps[:,1], c=\"g\", s=40, marker=\"X\", label=\"gt_kps\")\n",
    "    axes[1].set_title(\"Target Image\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- COMPUTE FINAL LOSS ON EVALUATION IMAGES ---\n",
    "# --- TAKE ALL EVALUATION IMAGES AND COMPUTE THE LOSS IN ORDER TO VERIFY THE UPDATE ---\n",
    "\n",
    "def compute_evaluation_loss():\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    dataset = SPair71kDataset(ALL_VAL_PATH, PATH_VAL)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)                # ANALIZE ALL VAL IMAGES\n",
    "    bar = tqdm(loader, desc=\"Computing evaluation loss\")\n",
    "\n",
    "    for batch in bar:\n",
    "        trg_kps = torch.tensor(batch[\"trg_kps\"][0], device=DEVICE, dtype=torch.float32)\n",
    "        pred_kps = match_keypoints(batch, grad=False)                  # NO GRADIENTS\n",
    "\n",
    "        loss = criterion(pred_kps, trg_kps)\n",
    "        total_loss += loss.item()\n",
    "        total_samples += 1                                      # COMPUTE LOSS\n",
    "\n",
    "    mean_loss = total_loss / total_samples\n",
    "    print(\"Mean evaluation loss: \", str(mean_loss))               # MEAN VALUE\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "# --- TRAINING FUNCTION. ---\n",
    "# --- DEFINE IMAGES, LOSS AND METRIC, THAN UNFREEZE LAST LAYERS ---\n",
    "# --- FOR EACH EPOCH, ANALYZE ALL TRAINING IMAGES AND UPLOAD WEIGHTS COMPUTING THE LOSS. ---\n",
    "# --- THEN EVALUATE THE MODEL ON THE EVALUATION IMAGES AND SAVE THE MODEL IN CASE IT'S THE BEST ONE. --- \n",
    "\n",
    "def Train(model, number_of_Epochs, learning_rate, layers):\n",
    "    global_loss = float('inf')\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    dataset = SPair71kDataset(ALL_TRAIN_PATH, PATH_TRAIN, None)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)           # INITIALIZATION\n",
    "    params = []\n",
    "\n",
    "    print()\n",
    "    print(\"Trying with\", layers, \"free layers\")\n",
    "    model = setup_light_finetuning(model, layers)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:                  # TRAINABLE PARAMETERS\n",
    "            params.append(p)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "    for epoch in range(number_of_Epochs):\n",
    "        model.train()\n",
    "        print()\n",
    "        print(\"Starting epoch\", epoch)\n",
    "        pbar = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "        for batch in pbar:\n",
    "            trg_kps = torch.tensor(batch[\"trg_kps\"][0], device=DEVICE, dtype=torch.float32)\n",
    "      \n",
    "            pred_kps = match_keypoints(batch, grad=True)     # PREDICTION WITH BASIC SOFTMAX\n",
    "            loss = criterion(pred_kps, trg_kps)           # LOSS\n",
    "\n",
    "            optimizer.zero_grad()           # UPDATING\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # UPDATING BAR AND GLOBAL LOSS\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # EVALUATION\n",
    "\n",
    "        model.eval()\n",
    "        loss = compute_evaluation_loss()\n",
    "\n",
    "        if loss < global_loss:\n",
    "            path = os.path.join(PATH_EXPORT, \"best_model.pth\")          # UPDATE ON DRIVE\n",
    "            torch.save(model.state_dict(), path)\n",
    "            global_loss = loss\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    return\n",
    "\n",
    "# --- EVALUATION FUNCTION ---\n",
    "# --- CONSIDER ONLY ONE CATEGORY. ---\n",
    "# --- FOR EACH PAIR: LOAD SOURCE AND TARGET IMAGES ---\n",
    "# --- EXTRACT DESCRIPTORS AND RESCALE COORDINATES ---\n",
    "# --- USE COSINE SIMILARITY IN ORDER TO FIND THE CORRESPONDING POINT IN THE TARGET IMAGE. ---\n",
    "# --- FINALLY, RETURN THE CORRECT GENERATED KEYPOINTS (USING THEIR DISTANCE FROM THE ORIGINAL ONE) RATIO. ---\n",
    "# --- OPTIONALLY: VISUALIZE RESULTS. ---\n",
    "\n",
    "def run_evaluation(category, result_path, visualize=False):\n",
    "    results_keypoints = {}\n",
    "    total_correct = {alpha: 0 for alpha in ALPHA}\n",
    "    total_points = 0\n",
    "\n",
    "    dataset_cat = SPair71kDataset(ALL_TEST_PATH, PATH_TEST, cat)\n",
    "    loader = DataLoader(dataset_cat, batch_size=1, shuffle=False)         # EVALUATE THIS CATEGORY OF IMAGES\n",
    "    pbar = tqdm(loader, desc=\"Evaluating \" + category)\n",
    "\n",
    "    for batch in pbar:\n",
    "        trg_kps = batch[\"trg_kps\"][0].numpy()\n",
    "        kps_ids = batch[\"kps_ids\"][0].numpy()\n",
    "        pred_kps = match_keypoints(batch, False).numpy()\n",
    "\n",
    "        max_dim = max(batch[\"trg_bndbox\"][0][2]-batch[\"trg_bndbox\"][0][0], batch[\"trg_bndbox\"][0][3]-batch[\"trg_bndbox\"][0][1])\n",
    "        total_correct_image = {alpha: 0 for alpha in ALPHA}\n",
    "        total_points_image = 0                                      # FOR THE CURRENT IMAGE\n",
    "\n",
    "        for (i,key) in enumerate(kps_ids):\n",
    "            if str(key) not in results_keypoints:             # IF HAS NOT BEEN CONSIDERED YET, ADD IT\n",
    "                results_keypoints[str(key)] = []\n",
    "\n",
    "            dist = np.linalg.norm(pred_kps[i] - trg_kps[i])            # DISTANCE METRIC\n",
    "            total_points += 1\n",
    "            total_points_image += 1\n",
    "\n",
    "            for alpha in ALPHA:\n",
    "                results_keypoints[str(key)].append((alpha, bool(dist <= alpha * max_dim)))\n",
    "\n",
    "                if dist <= alpha * max_dim:                     # PREDICTION IS CORRECT?\n",
    "                    total_correct[alpha] += 1\n",
    "                    total_correct_image[alpha] += 1\n",
    "\n",
    "        # PRINT PER IMAGE\n",
    "\n",
    "        print()\n",
    "\n",
    "        if USE_WIN:\n",
    "            print(\"Results for file \" + batch[\"file_name\"][0] + \" using softmax\")\n",
    "            print(\"Window size:\", WINDOW_SOFTMAX)\n",
    "            print(\"Tau:\", TAU_SOFTMAX)\n",
    "        else:\n",
    "            print(\"Results for file \" + batch[\"file_name\"][0] + \" using argmax\")\n",
    "\n",
    "        \n",
    "        print()\n",
    "\n",
    "        for alpha in ALPHA:\n",
    "            total_correct_image[alpha] = round(100 * total_correct_image[alpha] / total_points_image, 2)      # PCKS PER IMAGE\n",
    "            print(\"PCK@\" + str(alpha) + \": \" + str(total_correct_image[alpha]) + \"%\")\n",
    "\n",
    "        mean_pck_image = round(sum(total_correct_image.values()) / len(ALPHA), 2)\n",
    "        print(\"MEAN PCK:\", mean_pck_image, \"%\")\n",
    "        total_correct_image[\"MEAN\"] = mean_pck_image              # MEAN\n",
    "\n",
    "        # SAVE JSON\n",
    "\n",
    "        path = os.path.join(result_path, \"pck_percentages_for_\" + batch[\"file_name\"][0])\n",
    "        file = open(path, \"w\")\n",
    "        json.dump(total_correct_image, file, indent=4)\n",
    "        file.close()\n",
    "\n",
    "        # VISUALIZE\n",
    "\n",
    "        if visualize:\n",
    "            visualize_keypoints(batch[\"src_path\"][0], batch[\"trg_path\"][0], batch[\"src_kps\"][0], pred_kps, trg_kps)\n",
    "\n",
    "    # KEYPOINTS\n",
    "\n",
    "    path = os.path.join(result_path, \"keypoints_percentages_for_\" + category + \".json\")\n",
    "    file = open(path, \"w\")\n",
    "    json.dump(results_keypoints, file, indent=4)               # SAVE KEYPOINTS ANALYZIS\n",
    "    file.close()\n",
    "\n",
    "    # CATEGORY PCK\n",
    "\n",
    "    print()\n",
    "    print(\"=\"*40)\n",
    "    print(\"Category:\", category)                                   # FOR THE CURRENT CATEGORY\n",
    "    print()\n",
    "\n",
    "    for alpha in ALPHA:\n",
    "        total_correct[alpha] = round(100 * total_correct[alpha] / total_points, 2)         # COMPUTE ALPHAS PCKS\n",
    "        print(\"PCK@\" + str(alpha) + \": \" + str(total_correct[alpha]), \"%\")\n",
    "\n",
    "    mean_pck = round(sum(total_correct.values()) / len(ALPHA), 2)             # COMPUTE MEAN\n",
    "    total_correct[\"MEAN\"] = mean_pck\n",
    "    print(\"MEAN PCK: \" + str(mean_pck) + \"%\")\n",
    "\n",
    "    path = os.path.join(result_path, \"pck_mean_percentages_for_\" + category + \".json\")\n",
    "    file = open(path, \"w\")\n",
    "    json.dump(total_correct, file, indent=4)              # SAVE\n",
    "    file.close()\n",
    "    return\n",
    "\n",
    "# --- MOUNT DRIVE AND EXTRACT DATASET ---\n",
    "\n",
    "drive.mount(PATH_DRIVE, force_remount=True)\n",
    "!tar -xzf {PATH_FILE}\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "\n",
    "print()\n",
    "print(\"Loading \", MODEL_VERSION, \" model...\")\n",
    "\n",
    "if MODEL_VERSION == \"dinov2\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14_reg\", pretrained=True).to(DEVICE)\n",
    "elif MODEL_VERSION == \"dinov3\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov3\", \"dinov3_vitb16\", weights=PTH_PATH).to(DEVICE)\n",
    "elif MODEL_VERSION == \"sam\":\n",
    "    model = sam_model_registry[\"vit_b\"](checkpoint=PTH_PATH).to(DEVICE)\n",
    "    predictor = SamPredictor(model)\n",
    "\n",
    "# --- TRAINING IF ENABLED ---\n",
    "\n",
    "if TUNING:\n",
    "    Train(model, number_of_Epochs=5, learning_rate=1e-5, layers=3)\n",
    "\n",
    "    best_model = torch.load(PATH_EXPORT, map_location=DEVICE)              # LOADING BEST MODEL\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- EVALUATION PER CATEGORY ---\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    print()\n",
    "    print(\"Evaluating category: \", cat)\n",
    "\n",
    "    path = os.path.join(PATH_RES, MODEL_VERSION, cat)                    # NEW DIRECTORY\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    run_evaluation(cat, path, visualize=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
