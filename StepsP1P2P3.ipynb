{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69ba7f",
   "metadata": {
    "id": "0f69ba7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from google.colab import drive\n",
    "!pip install torchmetrics                # ONLY FOR FIRST EXECUTION\n",
    "\n",
    "# --- CONFIGURATIONS: SET PATHS, DEVICE, MODEL AND HYPERPARAMETERS ---\n",
    "\n",
    "PATH_DRIVE = \"/content/drive\"\n",
    "PATH_EXPORT = \"/content/drive/MyDrive\"\n",
    "PATH_FILE = \"/content/drive/MyDrive/SPair-71k.tar.gz\"\n",
    "PATH_DB = \"/content/SPair-71k\"\n",
    "\n",
    "PATH_TEST = \"/content/SPair-71k/PairAnnotation/test\"\n",
    "PATH_VAL = \"/content/SPair-71k/PairAnnotation/val\"\n",
    "PATH_TRAIN = \"/content/SPair-71k/PairAnnotation/trn\"\n",
    "\n",
    "ALL_TEST_PATH = \"/content/SPair-71k/Layout/small/test.txt\"\n",
    "ALL_TRAIN_PATH = \"/content/SPair-71k/Layout/small/trn.txt\"\n",
    "ALL_VAL_PATH = \"/content/SPair-71k/Layout/small/val.txt\"\n",
    "\n",
    "IMAGE_FOLDER_NAME = \"/content/SPair-71k/JPEGImages\"\n",
    "PTH_PATH = \"/content/drive/MyDrive/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\"\n",
    "\n",
    "PATH_RES = \"/content/Results\"\n",
    "os.makedirs(PATH_RES, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_VERSION = \"dinov2\"         # \"dinov2\", \"dinov3\", \"sam\"\n",
    "TUNING = True                     # TRAINING OR INFERENCE\n",
    "\n",
    "DICT_WINDOW_SIZE_TAU = {3: 0.01, 5: 0.05, 7: 0.07}\n",
    "IMAGE_SIZE = 1024 if MODEL_VERSION == \"sam\" else 224\n",
    "PATCH_SIZE = 14 if MODEL_VERSION == \"dinov2\" else 16\n",
    "H_PATCH, W_PATCH = IMAGE_SIZE // PATCH_SIZE, IMAGE_SIZE // PATCH_SIZE\n",
    "ALPHA = [0.05, 0.1, 0.2]\n",
    "CATEGORIES = [ \"aeroplane\" ] # , \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"dog\", \"horse\",\n",
    "                #  \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"train\", \"tvmonitor\" ]\n",
    "\n",
    "# --- IF SAM MODEL ---\n",
    "\n",
    "if MODEL_VERSION == \"sam\":\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "    from segment_anything import sam_model_registry, SamPredictor        # DOWNLOAD\n",
    "    PTH_PATH = \"/content/drive/MyDrive/sam_vit_b_01ec64.pth\"\n",
    "    predictor = None\n",
    "\n",
    "# --- DATASET CLASS ---\n",
    "\n",
    "class SPair71kDataset(Dataset):\n",
    "    def __init__(self, pair_path, source_path, category_filter=None):\n",
    "        self.pair_files = []\n",
    "        self.image_path = source_path\n",
    "        file = open(pair_path, \"r\")\n",
    "\n",
    "        for line in file:\n",
    "            (pair_id, category) = line.strip().split(\".json\")[0].split(\":\")\n",
    "            if TUNING or category == category_filter:\n",
    "                self.pair_files.append((line, category))\n",
    "\n",
    "        file.close()\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (json_file, category) = self.pair_files[idx]\n",
    "        json_path = os.path.join(self.image_path, json_file.strip() + \".json\")\n",
    "\n",
    "        file = open(json_path, \"r\")\n",
    "        annotation = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "        src_name = annotation[\"src_imname\"]\n",
    "        trg_name = annotation[\"trg_imname\"]\n",
    "        src_path = os.path.join(IMAGE_FOLDER_NAME, category, src_name)\n",
    "        trg_path = os.path.join(IMAGE_FOLDER_NAME, category, trg_name)\n",
    "        ids = [int(el) for el in annotation[\"kps_ids\"]]\n",
    "\n",
    "        return {\n",
    "            \"src_path\": src_path,\n",
    "            \"trg_path\": trg_path,\n",
    "            \"src_kps\": np.array(annotation[\"src_kps\"], dtype=np.float32),\n",
    "            \"trg_kps\": np.array(annotation[\"trg_kps\"], dtype=np.float32),\n",
    "            \"trg_bndbox\": np.array(annotation[\"trg_bndbox\"], dtype=np.float32),           # GET DATA\n",
    "            \"src_name\": src_name,\n",
    "            \"trg_name\": trg_name,\n",
    "            \"kps_ids\": np.array(ids, dtype=np.int32)\n",
    "        }\n",
    "\n",
    "# --- IMAGE PREPROCESSING ---\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "# UNFREEZE ONLY THE LAST num_last_blocks LAYERS AND THE FINAL LAYER NORM\n",
    "\n",
    "def setup_light_finetuning(model, num_last_blocks):\n",
    "    N_Params = 0\n",
    "    N_Free_Params = 0\n",
    "\n",
    "    if MODEL_VERSION == \"sam\":\n",
    "        blocks_to_unfreeze = model.image_encoder.blocks[-num_last_blocks:]\n",
    "        \n",
    "        if hasattr(model.image_encoder, \"post_norm\"):       # NOT SURE THE FINAL NORM IS ACCESSIBLE\n",
    "            norm = model.image_encoder.post_norm\n",
    "        else:\n",
    "            norm = None\n",
    "\n",
    "    else:\n",
    "        blocks_to_unfreeze = model.blocks[-num_last_blocks:]\n",
    "        norm = model.norm\n",
    "\n",
    "    for param in model.parameters():                # FREEZE ALL\n",
    "        N_Params += param.numel()\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for block in blocks_to_unfreeze:\n",
    "\n",
    "        for param in block.parameters():\n",
    "            N_Free_Params += param.numel()           # UNFREEZE\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if norm:\n",
    "        for param in norm.parameters():          # UNFREEZE NORM\n",
    "            N_Free_Params += param.numel()\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # NUMBERS\n",
    "\n",
    "    print(\"Total parameters:\", N_Params)\n",
    "    print(\"Total trainable:\", N_Free_Params)\n",
    "    print(\"Percentage trainable:\", round(100 * N_Free_Params / N_Params, 2), \"%\")\n",
    "    return model\n",
    "\n",
    "# --- LOAD AND PREPROCESS IMAGE. EXTRACT PATCH-WISE FEATURES FROM MODEL. ---\n",
    "# --- RESHAPE AND NORMALIZE FEATURE MAPS. ---\n",
    "# --- FINALLY, RETURN THE EXTRACTED FEATURE MAP AND THE ORIGINAL DIMENSIONS. --\n",
    "\n",
    "def get_descriptors(img_path, grad):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    (w, h) = img.size\n",
    "    input_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.set_grad_enabled(grad):\n",
    "\n",
    "        if MODEL_VERSION == \"dinov2\":\n",
    "            feats = model.get_intermediate_layers(input_tensor, n=1)[0]\n",
    "            feats = feats.reshape(1, H_PATCH, W_PATCH, feats.shape[2])\n",
    "\n",
    "        elif MODEL_VERSION == \"dinov3\":\n",
    "            x = model.forward_features(input_tensor)[\"x_norm_patchtokens\"]\n",
    "            feats = x.reshape(1, H_PATCH, W_PATCH, x.shape[-1])\n",
    "\n",
    "        elif MODEL_VERSION == \"sam\":\n",
    "\n",
    "            if TUNING:\n",
    "                feats = model.image_encoder(input_tensor)[:, 1:, :]            # FOR TRAINING\n",
    "            else:\n",
    "                predictor.set_image(np.array(img))\n",
    "                feats = predictor.get_image_embedding()                 # FOR INFERENCE\n",
    "                feats = feats.permute(0, 2, 3, 1)\n",
    "\n",
    "    feats = F.normalize(feats, dim=-1)\n",
    "    return (feats, w, h)\n",
    "\n",
    "# --- MATCH KEYPOINTS. ---\n",
    "# --- EXTRACT FEATURE SIZE, CREATE PROBABILITY GRID FOR SOFT ARG MAX\n",
    "\n",
    "def match_keypoints(batch, grad, tau=0.05, use_dict=False):\n",
    "    src_kps = batch[\"src_kps\"][0].numpy()\n",
    "\n",
    "    (feat_src, sw, sh) = get_descriptors(batch[\"src_path\"][0], grad)          # GET DESCRIPTORS\n",
    "    (feat_trg, tw, th) = get_descriptors(batch[\"trg_path\"][0] , grad)\n",
    "\n",
    "    (_, Hf, Wf, D) = feat_trg.shape\n",
    "    trg_flat = feat_trg[0].reshape(Hf * Wf, D)\n",
    "    pred_kps = []\n",
    "\n",
    "    for i in range(src_kps.shape[0]):\n",
    "        sx = int(src_kps[i, 0] * Wf / sw)\n",
    "        sy = int(src_kps[i, 1] * Hf / sh)\n",
    "        sx = torch.clamp(torch.tensor(sx, device=DEVICE), 0, Wf - 1)\n",
    "        sy = torch.clamp(torch.tensor(sy, device=DEVICE), 0, Hf - 1)\n",
    "        src_desc = feat_src[0, sy, sx, :]\n",
    "\n",
    "        sim = torch.matmul(trg_flat, src_desc)            # COSINE SIMILARITY\n",
    "\n",
    "        if TUNING:\n",
    "            sim_map = sim.reshape(Hf, Wf)            # TUNING PHASE\n",
    "\n",
    "            if use_dict:\n",
    "                predicted_values = []\n",
    "\n",
    "                for (k,v) in DICT_WINDOW_SIZE_TAU.items():\n",
    "                    pred_xy = window_soft_argmax(sim_map, k, v)              # WINDOW SOFTMAX\n",
    "                    (px, py) = (pred_xy[0], pred_xy[1])\n",
    "\n",
    "                    predicted_values.append((px, py))         # FOR PLOTTING\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(torch.arange(Wf, device=DEVICE),\n",
    "                                                    torch.arange(Hf, device=DEVICE), indexing='ij'), dim=-1).reshape(-1,2)\n",
    "                prob = F.softmax(sim / tau, dim=0)\n",
    "                pred_xy = (coords.float() * prob[:, None]).sum(dim=0)\n",
    "        else:\n",
    "            best_idx = sim.argmax()\n",
    "            pred_xy = torch.tensor([best_idx % Wf, best_idx // Wf], device=DEVICE)\n",
    "\n",
    "        pred_x = (pred_xy[0] + 0.5) * (tw / Wf)\n",
    "        pred_y = (pred_xy[1] + 0.5) * (th / Hf)\n",
    "        pred_kps.append(torch.stack([pred_x, pred_y]))\n",
    "    \n",
    "    return torch.stack(pred_kps)\n",
    "\n",
    "# --- USE SOFTARGMAX TO PREDICT THE FINAL VALUE- ---  \n",
    "\n",
    "def window_soft_argmax(similarity_map, window_size, tau):\n",
    "    (H, W) = similarity_map.shape\n",
    "\n",
    "    best_idx = similarity_map.argmax()          # NORMAL SIMLARITY\n",
    "    y_peak = best_idx // W\n",
    "    x_peak = best_idx % W\n",
    "\n",
    "    half = window_size // 2\n",
    "    y0 = max(y_peak - half, 0)\n",
    "    y1 = min(y_peak + half + 1, H)              # WINDOW'S DIMENSIONS\n",
    "    x0 = max(x_peak - half, 0)\n",
    "    x1 = min(x_peak + half + 1, W)\n",
    "\n",
    "    window = similarity_map[y0:y1, x0:x1]           # SOFTMAX WINDOW\n",
    "\n",
    "    (ys, xs) = torch.meshgrid(torch.arange(y0, y1, device=similarity_map.device),\n",
    "                            torch.arange(x0, x1, device=similarity_map.device),\n",
    "                            indexing=\"ij\")\n",
    "    coords = torch.stack([xs.flatten(), ys.flatten()], dim=1).float()\n",
    "\n",
    "    prob = F.softmax(window.flatten() / tau, dim=0)        # NEW SOFTMAX\n",
    "    pred_patch = (coords * prob[:, None]).sum(dim=0)         # FINAL PREDICTION\n",
    "\n",
    "    return pred_patch\n",
    "\n",
    "\n",
    "# --- VISUALIZE RESULTS AND COMPARE CORRECT AND PREDICTED KEYPOINTS ON TARGET IMAGE. ---\n",
    "\n",
    "def visualize_keypoints(src_path, trg_path, src_kps, pred_kps, trg_kps):\n",
    "    src_img = cv2.imread(src_path)[:, :, ::-1]\n",
    "    trg_img = cv2.imread(trg_path)[:, :, ::-1]\n",
    "\n",
    "    (_, axes) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(src_img)\n",
    "    axes[0].scatter(src_kps[:,0], src_kps[:,1], c=\"r\", s=40, label=\"src_kps\")\n",
    "    axes[0].set_title(\"Source Image\")\n",
    "\n",
    "    axes[1].imshow(trg_img)\n",
    "    axes[1].scatter(pred_kps[:,0], pred_kps[:,1], c=\"b\", s=40, label=\"pred_kps\")\n",
    "    axes[1].scatter(trg_kps[:,0], trg_kps[:,1], c=\"g\", s=40, marker=\"X\", label=\"gt_kps\")\n",
    "    axes[1].set_title(\"Target Image\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- COMPUTE FINAL LOSS ON EVALUATION IMAGES ---\n",
    "\n",
    "def compute_evaluation_loss():\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    dataset = SPair71kDataset(ALL_VAL_PATH, PATH_VAL)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False)                # ANALIZE ALL VAL IMAGES\n",
    "    bar = tqdm(loader, desc=\"Computing evaluation loss\")\n",
    "\n",
    "    for batch in bar:\n",
    "        trg_kps = torch.tensor(batch[\"trg_kps\"][0], device=DEVICE, dtype=torch.float32)\n",
    "        pred_kps = match_keypoints(batch, grad=False, use_dict=True)                  # NO GRADIENTS\n",
    "\n",
    "        loss = criterion(pred_kps, trg_kps)\n",
    "        total_loss += loss.item()\n",
    "        total_samples += 1                                      # COMPUTE LOSS\n",
    "\n",
    "    mean_loss = total_loss / total_samples\n",
    "    print(\"Mean evaluation loss: \", str(mean_loss))               # MEAN VALUE\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "# --- TRAINING FUNCTION. ---\n",
    "\n",
    "def Train(model, number_of_Epochs, learning_rate, layers, tau=0.05):\n",
    "    global_loss = float('inf')\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    dataset = SPair71kDataset(ALL_TRAIN_PATH, PATH_TRAIN, None)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)           # INITIALIZATION\n",
    "    params = []\n",
    "\n",
    "    print()\n",
    "    print(\"Trying with\", layers, \"free layers\")\n",
    "    model = setup_light_finetuning(model, layers)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:                  # TRAINABLE PARAMETERS\n",
    "            params.append(p)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "    for epoch in range(number_of_Epochs):\n",
    "        model.train()\n",
    "        print()\n",
    "        print(\"Starting epoch\", epoch)\n",
    "        pbar = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "        for batch in pbar:\n",
    "            trg_kps = torch.tensor(batch[\"trg_kps\"][0], device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "            pred_kps = match_keypoints(batch, grad=True, tau=tau)     # PREDICTION WITH SOFTMAX\n",
    "            loss = criterion(pred_kps, trg_kps)           # LOSS\n",
    "\n",
    "            optimizer.zero_grad()           # UPDATING\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # UPDATING BAR AND GLOBAL LOSS\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # EVALUATION\n",
    "\n",
    "        model.eval()\n",
    "        loss = compute_evaluation_loss()\n",
    "\n",
    "        if loss < global_loss:\n",
    "            path = os.path.join(PATH_EXPORT, \"best_model.pth\")          # UPDATE ON DRIVE\n",
    "            torch.save(model.state_dict(), path)\n",
    "            global_loss = loss\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    return\n",
    "\n",
    "# --- EVALUATION FUNCTION ---\n",
    "# --- CONSIDER ONLY ONE CATEGORY. ---\n",
    "# --- FOR EACH PAIR: LOAD SOURCE AND TARGET IMAGES ---\n",
    "# --- EXTRACT DESCRIPTORS AND CORRECT KEYPOINTS AND RESCALE COORDINATES ---\n",
    "# --- FOR EACH ALPHA: USE COSINE SIMILARITY IN ORDER TO FIND THE CORRESPONDING POINT IN THE TARGET IMAGE. ---\n",
    "# --- FINALLY, RETURN THE CORRECT GENERATED KEYPOINTS (USING THEIR DISTANCE FROM THE ORIGINAL ONE) RATIO. ---\n",
    "# --- OPTIONALLY: VISUALIZE RESULTS. ---\n",
    "\n",
    "def run_evaluation(loader, category, result_path, visualize=False):\n",
    "    results_keypoints = {}\n",
    "    total_correct = {alpha: 0 for alpha in ALPHA}\n",
    "    total_points = 0\n",
    "    pbar = tqdm(loader, desc=\"Evaluating \" + category)\n",
    "\n",
    "    for batch in pbar:\n",
    "        src_kps = batch[\"src_kps\"][0].numpy()\n",
    "        trg_kps = batch[\"trg_kps\"][0].numpy()\n",
    "        kps_ids = batch[\"kps_ids\"][0].numpy()\n",
    "\n",
    "        pred_kps = match_keypoints(batch, False).numpy()\n",
    "\n",
    "        max_dim = max(batch[\"trg_bndbox\"][0][2]-batch[\"trg_bndbox\"][0][0], batch[\"trg_bndbox\"][0][3]-batch[\"trg_bndbox\"][0][1])\n",
    "        total_correct_image = {alpha: 0 for alpha in ALPHA}\n",
    "        total_points_image = 0                                      # FOR THE CURRENT IMAGE\n",
    "\n",
    "        for i in range(len(src_kps)):\n",
    "            if str(kps_ids[i]) not in results_keypoints:\n",
    "                results_keypoints[str(kps_ids[i])] = []\n",
    "\n",
    "            dist = np.linalg.norm(pred_kps[i] - trg_kps[i])\n",
    "            total_points += 1\n",
    "            total_points_image += 1\n",
    "\n",
    "            for alpha in ALPHA:\n",
    "                results_keypoints[str(kps_ids[i])].append((alpha, bool(dist <= alpha * max_dim)))\n",
    "\n",
    "                if dist <= alpha * max_dim:                     # PREDICTION IS CORRECT?\n",
    "                    total_correct[alpha] += 1\n",
    "                    total_correct_image[alpha] += 1\n",
    "\n",
    "        # PRINT PER IMAGE\n",
    "\n",
    "        print()\n",
    "        print(\"Results for (\", batch[\"src_name\"][0], \",\", batch[\"trg_name\"][0], \")\")\n",
    "        print()\n",
    "\n",
    "        for alpha in ALPHA:\n",
    "            total_correct_image[alpha] = round(100 * total_correct_image[alpha] / total_points_image, 2)\n",
    "            print(\"PCK@\" + str(alpha) + \": \" + str(total_correct_image[alpha]) + \"%\")\n",
    "\n",
    "        mean_pck_image = round(sum(total_correct_image.values()) / len(ALPHA), 2)\n",
    "        print(\"MEAN PCK:\", mean_pck_image, \"%\")\n",
    "        total_correct_image[\"MEAN\"] = mean_pck_image\n",
    "\n",
    "        # SAVE JSON\n",
    "\n",
    "        path = os.path.join(result_path, \"pck_results_for_\" + batch[\"src_name\"][0] + \"_\" + batch[\"trg_name\"][0] + \".json\")\n",
    "        file = open(path, \"w\")\n",
    "        json.dump(total_correct_image, file, indent=4)\n",
    "        file.close()\n",
    "\n",
    "        # VISUALIZE\n",
    "\n",
    "        if visualize:\n",
    "            visualize_keypoints(batch[\"src_path\"][0], batch[\"trg_path\"][0], src_kps, pred_kps, trg_kps)\n",
    "\n",
    "    # KEYPOINTS\n",
    "\n",
    "    path = os.path.join(result_path, \"keypoints_results_for_\" + category + \".json\")\n",
    "    file = open(path, \"w\")\n",
    "    json.dump(results_keypoints, file, indent=4)\n",
    "    file.close()\n",
    "\n",
    "    # CATEGORY PCK\n",
    "\n",
    "    print()\n",
    "    print(\"=\"*40)\n",
    "    print(\"Category:\", category)\n",
    "    print()\n",
    "\n",
    "    for alpha in ALPHA:\n",
    "        total_correct[alpha] = round(100 * total_correct[alpha] / total_points, 2)\n",
    "        print(\"PCK@\", str(alpha), \": \", str(total_correct[alpha]), \"%\")\n",
    "\n",
    "    mean_pck = round(sum(total_correct.values()) / len(ALPHA), 2)\n",
    "    total_correct[\"MEAN\"] = mean_pck\n",
    "    print(\"MEAN PCK :\" + str(mean_pck) + \"%\")\n",
    "\n",
    "    path = os.path.join(result_path, \"pck_mean_results_for_\" + category + \".json\")\n",
    "    file = open(path, \"w\")\n",
    "    json.dump(total_correct, file, indent=4)\n",
    "    file.close()\n",
    "    return\n",
    "\n",
    "# --- MOUNT DRIVE AND EXTRACT DATASET ---\n",
    "\n",
    "drive.mount(PATH_DRIVE, force_remount=True)\n",
    "!tar -xzf {PATH_FILE}\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "\n",
    "print()\n",
    "print(\"Loading \", MODEL_VERSION, \" model...\")\n",
    "\n",
    "if MODEL_VERSION == \"dinov2\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14_reg\", pretrained=True).to(DEVICE)\n",
    "elif MODEL_VERSION == \"dinov3\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov3\", \"dinov3_vitb16\", weights=PTH_PATH).to(DEVICE)\n",
    "elif MODEL_VERSION == \"sam\":\n",
    "    model = sam_model_registry[\"vit_b\"](checkpoint=PTH_PATH).to(DEVICE)\n",
    "    predictor = SamPredictor(model)\n",
    "\n",
    "# --- TRAINING IF ENABLED ---\n",
    "\n",
    "if TUNING:\n",
    "    Train(model, number_of_Epochs=5, learning_rate=1e-5, layers=3)\n",
    "    path = os.path.join(PATH_EXPORT, \"best_model.pth\")\n",
    "    \n",
    "    best_model = torch.load(path, map_location=DEVICE)              # LOADING BEST MODEL\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- EVALUATION PER CATEGORY ---\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    print()\n",
    "    print(\"Evaluating category: \", cat)\n",
    "\n",
    "    path = os.path.join(PATH_RES, MODEL_VERSION, cat)                    # NEW DIRECTORY\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    dataset_cat = SPair71kDataset(ALL_TEST_PATH, PATH_TEST, cat)\n",
    "    loader = DataLoader(dataset_cat, batch_size=1, shuffle=False)\n",
    "    run_evaluation(loader, cat, path, visualize=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
