{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69ba7f",
   "metadata": {
    "id": "0f69ba7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from google.colab import drive\n",
    "!pip install torchmetrics                 # NEEDED IN THE FIRST EXECUTION\n",
    "\n",
    "# --- CONFIGURATIONS: SET PATHS, DIVICE, MODEL AND HYPERPARAMETERS ---\n",
    "\n",
    "PATH_DRIVE = \"/content/drive\"\n",
    "PATH_FILE = \"/content/drive/MyDrive/SPair-71k.tar.gz\"\n",
    "PATH_DB = \"/content/SPair-71k\"\n",
    "PATH_TEST = \"/content/SPair-71k/PairAnnotation/test\"\n",
    "IMAGE_FOLDER_NAME = \"/content/SPair-71k/JPEGImages\"\n",
    "ALL_PAIRS_PATH = \"/content/SPair-71k/Layout/small/test.txt\"\n",
    "PTH_PATH = \"/content/drive/MyDrive/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\"\n",
    "\n",
    "PATH_RES = \"/content/Results\"\n",
    "os.makedirs(PATH_RES, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"          # MODEL IN [dinov2, dinov3, sam]\n",
    "MODEL_VERSION = \"dinov2\"\n",
    "\n",
    "if MODEL_VERSION == \"sam\":\n",
    "      !pip install git+https://github.com/facebookresearch/segment-anything.git             # FOR SAM\n",
    "      from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "      PTH_PATH = \"/content/drive/MyDrive/sam_vit_b_01ec64.pth\"\n",
    "      predictor = None\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "PATCH_SIZE = 14 if MODEL_VERSION == \"dinov2\" else 16\n",
    "(H_PATCH, W_PATCH) = (IMAGE_SIZE // PATCH_SIZE, IMAGE_SIZE // PATCH_SIZE)\n",
    "ALPHA = [0.05, 0.1, 0.2]\n",
    "CATEGORIES = [\"aeroplane\"] # , \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"dog\", \"horse\",\n",
    "              # \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# --- PREPROCESSING IMAGES: RESIZE, CONVERT INTO TENSOR AND NORMALIZE ACCORDING TO IMAGENET. ---\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- HELPER FUNCTIONS. ---\n",
    "\n",
    "# --- LOAD AND PREPROCESS IMAGE. EXTRACT PATCH-WISE FEATURES FROM DINO MODEL. ---\n",
    "# --- RESHAPE AND NORMALIZE FEATURE MAPS. ---\n",
    "# --- FINALLY, RETURN THE EXTRACTED FEATURE MAP AND THE ORIGINAL DIMENSIONS. ---\n",
    "\n",
    "def get_descriptors(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    (w, h) = img.size\n",
    "    input_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        if MODEL_VERSION == \"dinov2\":\n",
    "            feats = model.get_intermediate_layers(input_tensor, n=1)[0]  # [1,N,D]\n",
    "            feats = feats.reshape(1, H_PATCH, W_PATCH, feats.shape[2])   # PATCH-WISE\n",
    "\n",
    "        elif MODEL_VERSION == \"dinov3\":\n",
    "            x = model.forward_features(input_tensor)[\"x_norm_patchtokens\"]\n",
    "            feats = x.reshape(1, H_PATCH, W_PATCH, x.shape[-1])  # [1,H,W,D]\n",
    "\n",
    "        elif MODEL_VERSION == \"sam\":\n",
    "            predictor.set_image(np.array(img))            # CONVERT INTO ARRAY\n",
    "            feats = predictor.get_image_embedding()   # [1, D, H', W']\n",
    "            feats = feats.permute(0, 2, 3, 1)        # [1, H', W', D]\n",
    "\n",
    "    feats = F.normalize(feats, dim=-1)\n",
    "    return (feats, w, h)\n",
    "\n",
    "# --- LOAD IMAGES AND VISUALIZE SOURCE KEYPOINTS (RED), TARGET KEYPOINTS (GREEN) AND TARGET PREDICTED KEYPOINTS (BLUE). ---\n",
    "\n",
    "def visualize_keypoints(src_path, trg_path, src_kps, pred_kps, trg_kps):\n",
    "    src_img = cv2.imread(src_path)[:,:,::-1] # BGR -> RGB\n",
    "    trg_img = cv2.imread(trg_path)[:,:,::-1]\n",
    "\n",
    "    (fig, axes) = plt.subplots(1,2, figsize=(12,6))\n",
    "    axes[0].imshow(src_img)\n",
    "    axes[0].scatter(src_kps[:,0], src_kps[:,1], c=\"r\", s=40, label=\"src_kps\")          # SOURCE\n",
    "    axes[0].set_title(\"Source Image\")\n",
    "\n",
    "    axes[1].imshow(trg_img)\n",
    "    axes[1].scatter(pred_kps[:,0], pred_kps[:,1], c=\"b\", s=40, label=\"pred_kps\")\n",
    "    axes[1].scatter(trg_kps[:,0], trg_kps[:,1], c=\"g\", s=40, marker=\"X\", label=\"gt_kps\")       # CORRECT AND PREDICTED\n",
    "    axes[1].set_title(\"Target Image\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "# --- SHOW TWO IMAGES (SOURCE AND TARGET) WITH KEYPOINTS. ---\n",
    "# --- READS THE LIST OF TEST PAIRS AND FILTER FOR THE CURRENT CATEGORY. ---\n",
    "# --- FOR EACH PAIR: LOAD SOURCE AND TARGET IMAGES, EXTRACT DESCRIPTORS AND CORRECT KEYPOINTS AND RESCALE COORDINATES ---\n",
    "# --- FOR EACH ALPHA: USE COSINE SIMILARITY IN ORDER TO FIND THE CORRESPONDING POINT IN THE TARGET IMAGE. ---\n",
    "# --- FINALLY, RETURN THE CORRECT GENERATED KEYPOINTS (USING THEIR DISTANCE FROM THE ORIGINAL ONE) RATIO. ---\n",
    "# --- OPTIONALLY: VISUALIZE RESULTS. ---\n",
    "\n",
    "def run_evaluation(category, visualize=False):\n",
    "    total_correct = {alpha: 0 for alpha in ALPHA}\n",
    "    total_points = 0\n",
    "    category_filenames = []\n",
    "    file = open(ALL_PAIRS_PATH, \"r\")\n",
    "\n",
    "    for line in file:\n",
    "        parts = line.strip().split(':')\n",
    "\n",
    "        if parts[1] == category:\n",
    "            filename = line.strip() + \".json\"\n",
    "            path = os.path.join(PATH_TEST, filename)        # FILTER\n",
    "            if os.path.exists(path):\n",
    "                category_filenames.append(path)\n",
    "\n",
    "    file.close()\n",
    "    pbar = tqdm(category_filenames, desc=\"Evaluating \" + category)\n",
    "\n",
    "    for filename in pbar:\n",
    "        total_correct_image = {alpha: 0 for alpha in ALPHA}\n",
    "        total_points_image = 0\n",
    "\n",
    "        file = open(filename, \"r\")\n",
    "        annotation = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "        (src_name, trg_name) = (annotation[\"src_imname\"], annotation[\"trg_imname\"])\n",
    "        src_path = os.path.join(IMAGE_FOLDER_NAME, category, src_name)\n",
    "        trg_path = os.path.join(IMAGE_FOLDER_NAME, category, trg_name)\n",
    "\n",
    "        (feat_src, sw, sh) = get_descriptors(src_path)\n",
    "        (feat_trg, tw, th) = get_descriptors(trg_path)         # DESCRIPTORS\n",
    "\n",
    "        src_kps = np.array(annotation[\"src_kps\"])\n",
    "        trg_kps = np.array(annotation[\"trg_kps\"])                   # KEYPOINTS AND THRESHOLD\n",
    "        trg_bbox = np.array(annotation[\"trg_bndbox\"])\n",
    "        max_dim = max(trg_bbox[2]-trg_bbox[0], trg_bbox[3]-trg_bbox[1])\n",
    "\n",
    "        pred_kps = []\n",
    "        (_, Hf, Wf, D) = feat_trg.shape\n",
    "        trg_flat = feat_trg[0].reshape(Hf*Wf, D)\n",
    "        scale_x = Wf / sw\n",
    "        scale_y = Hf / sh\n",
    "        sx = np.clip((src_kps[:,0] * scale_x).astype(int), 0, Wf-1)          # RESCALING TO FEATURE MAP\n",
    "        sy = np.clip((src_kps[:,1] * scale_y).astype(int), 0, Hf-1)\n",
    "\n",
    "        for i in range(len(src_kps)):\n",
    "            src_desc = feat_src[0, sy[i], sx[i]]\n",
    "\n",
    "            # COSINE SIMILARITY\n",
    "\n",
    "            sim = torch.matmul(trg_flat, src_desc)\n",
    "            best_idx = sim.argmax()\n",
    "\n",
    "            px = best_idx % Wf\n",
    "            py = best_idx // Wf\n",
    "\n",
    "            pred_x = (px + 0.5) * (tw / Wf)\n",
    "            pred_y = (py + 0.5) * (th / Hf)\n",
    "            pred_kps.append([pred_x, pred_y])\n",
    "\n",
    "            # EUCLIDEAN DISTANCE TO VERIFY\n",
    "\n",
    "            dist = np.linalg.norm([pred_x - trg_kps[i,0], pred_y - trg_kps[i,1]])\n",
    "            total_points += 1\n",
    "            total_points_image += 1\n",
    "\n",
    "            for alpha in ALPHA:\n",
    "                if dist <= alpha * max_dim:\n",
    "                    total_correct[alpha] += 1\n",
    "                    total_correct_image[alpha] += 1\n",
    "\n",
    "        # UPDATE PROGRES BAR FOR THE CURRENT IMAGE\n",
    "\n",
    "        str_bar = {}\n",
    "        print()\n",
    "        print(\"Results for the couple (\", src_name, \",\", trg_name, \")\")\n",
    "        print()\n",
    "\n",
    "        for alpha in ALPHA:\n",
    "            str_bar[alpha] = round(100 * total_correct_image[alpha] / total_points_image, 2)\n",
    "            print(\"PCK for alpha=\", alpha, \" --> \", str_bar[alpha] ,\"%\")\n",
    "\n",
    "        mean_pck_image = sum(str_bar.values()) / len(str_bar)\n",
    "        print(\"MEAN PCK: \", str(round(mean_pck_image,2)), \"%\")            # MEAN FOR IMAGE\n",
    "        str_bar[\"MEAN\"] = mean_pck_image\n",
    "\n",
    "        path = PATH_RES + \"/pck_results_for_\" + MODEL_VERSION + \"_\" + src_name + \"_\" + trg_name + \".json\"\n",
    "        file = open(path, \"w\")\n",
    "        json.dump(str_bar, file, indent=4)                  # SAVE FOR IMAGE\n",
    "        file.close()\n",
    "\n",
    "        # IMAGE VISUALIZATION\n",
    "\n",
    "        if visualize:\n",
    "            visualize_keypoints(src_path, trg_path, src_kps, np.array(pred_kps), trg_kps)\n",
    "\n",
    "    # PCK\n",
    "    print()\n",
    "    print(\"Results for\", category)\n",
    "    print()\n",
    "\n",
    "    for alpha in ALPHA:\n",
    "        total_correct[alpha] = round(100 * total_correct[alpha] / total_points, 2)\n",
    "        print(\"PCK for alpha=\", alpha, \" --> \", total_correct[alpha] ,\"%\")\n",
    "\n",
    "    return total_correct\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------- #\n",
    "\n",
    "# --- MOUNT DRIVE AND EXTRACT DATASET. ---\n",
    "\n",
    "drive.mount(PATH_DRIVE, force_remount=True)\n",
    "!tar -xzf {PATH_FILE}\n",
    "\n",
    "# --- LOAD MODEL BASED ON CHOICE. ---\n",
    "\n",
    "print()\n",
    "print(\"Loading \", MODEL_VERSION, \" model...\")\n",
    "\n",
    "if MODEL_VERSION == \"dinov2\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14_reg\", pretrained = True).to(DEVICE)\n",
    "\n",
    "elif MODEL_VERSION == \"dinov3\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov3\", \"dinov3_vitb16\", weights = PTH_PATH).to(DEVICE)\n",
    "\n",
    "elif MODEL_VERSION == \"sam\":\n",
    "    model = sam_model_registry[\"vit_b\"](checkpoint=PTH_PATH).to(DEVICE)\n",
    "    predictor = SamPredictor(model)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- LOOP AMONG CLASSES. ---\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    print(\"\\n\")\n",
    "    print(\"Evaluating category: \", cat)\n",
    "    results = run_evaluation(cat, visualize=False)\n",
    "    path = PATH_RES + \"/pck_results_for_\" + MODEL_VERSION + \"_\" + cat + \".json\"\n",
    "\n",
    "    mean_pck = sum(results.values()) / len(results)\n",
    "    print(\"=\"*40)\n",
    "    print(\"MEAN PCK for \", cat, \": \", str(round(mean_pck,2)), \"%\")            # MEAN\n",
    "    results[\"MEAN\"] = mean_pck\n",
    "\n",
    "    file = open(path, \"w\")\n",
    "    json.dump(results, file, indent=4)                  # SAVE MEAN\n",
    "    file.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
