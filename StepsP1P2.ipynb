{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69ba7f",
   "metadata": {
    "id": "0f69ba7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from google.colab import drive\n",
    "!pip install torchmetrics                # ONLY FOR FIRST EXECUTION\n",
    "\n",
    "# --- CONFIGURATIONS: SET PATHS, DEVICE, MODEL AND HYPERPARAMETERS ---\n",
    "\n",
    "PATH_DRIVE = \"/content/drive\"\n",
    "PATH_EXPORT = \"/content/drive/MyDrive\"\n",
    "PATH_FILE = \"/content/drive/MyDrive/SPair-71k.tar.gz\"\n",
    "PATH_DB = \"/content/SPair-71k\"\n",
    "PATH_TEST = \"/content/SPair-71k/PairAnnotation/test\"\n",
    "PATH_TRAIN = \"/content/SPair-71k/PairAnnotation/trn\"\n",
    "ALL_PAIRS_PATH = \"/content/SPair-71k/Layout/small/test.txt\"\n",
    "ALL_TRAIN_PAIRS_PATH = \"/content/SPair-71k/Layout/small/trn.txt\"\n",
    "\n",
    "IMAGE_FOLDER_NAME = \"/content/SPair-71k/JPEGImages\"\n",
    "PTH_PATH = \"/content/drive/MyDrive/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\"\n",
    "\n",
    "PATH_RES = \"/content/Results\"\n",
    "os.makedirs(PATH_RES, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"          \n",
    "MODEL_VERSION = \"dinov2\"         # \"dinov2\", \"dinov3\", \"sam\"\n",
    "TUNING = True                     # TRAINING OR INFERENCE\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "PATCH_SIZE = 14 if MODEL_VERSION == \"dinov2\" else 16\n",
    "H_PATCH, W_PATCH = IMAGE_SIZE // PATCH_SIZE, IMAGE_SIZE // PATCH_SIZE\n",
    "ALPHA = [0.05, 0.1, 0.2]\n",
    "CATEGORIES = [\"aeroplane\"]\n",
    "\n",
    "# --- IF SAM MODEL ---\n",
    "\n",
    "if MODEL_VERSION == \"sam\":\n",
    "    !pip install git+https://github.com/facebookresearch/segment-anything.git \n",
    "    from segment_anything import sam_model_registry, SamPredictor        # DOWNLOAD\n",
    "    PTH_PATH = \"/content/drive/MyDrive/sam_vit_b_01ec64.pth\"\n",
    "    predictor = None\n",
    "\n",
    "# --- DATASET CLASS ---\n",
    "\n",
    "class SPair71kDataset(Dataset):\n",
    "    def __init__(self, category_filter=None):\n",
    "        self.pair_files = []\n",
    "        self.path = ALL_TRAIN_PAIRS_PATH if TUNING else ALL_PAIRS_PATH\n",
    "        self.image_path = PATH_TRAIN if TUNING else PATH_TEST\n",
    "        file = open(self.path, \"r\")   # os.listdir(self.path)\n",
    "\n",
    "        for line in file:\n",
    "            (pair_id, category) = line.strip().split(\".json\")[0].split(\":\")\n",
    "            if TUNING or category == category_filter:\n",
    "                self.pair_files.append((line, category))\n",
    "\n",
    "        file.close()        \n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (json_file, category) = self.pair_files[idx]\n",
    "        json_path = os.path.join(self.image_path, json_file.strip() + \".json\")\n",
    "        \n",
    "        file = open(json_path, \"r\")\n",
    "        annotation = json.load(file)\n",
    "        file.close()\n",
    "\n",
    "        src_name = annotation[\"src_imname\"]\n",
    "        trg_name = annotation[\"trg_imname\"]\n",
    "        src_path = os.path.join(IMAGE_FOLDER_NAME, category, src_name)\n",
    "        trg_path = os.path.join(IMAGE_FOLDER_NAME, category, trg_name)\n",
    "\n",
    "        return {\n",
    "            \"src_path\": src_path,\n",
    "            \"trg_path\": trg_path,\n",
    "            \"src_kps\": np.array(annotation[\"src_kps\"]),\n",
    "            \"trg_kps\": np.array(annotation[\"trg_kps\"]),\n",
    "            \"trg_bndbox\": np.array(annotation[\"trg_bndbox\"]),           # GET DATA\n",
    "            \"src_name\": src_name,\n",
    "            \"trg_name\": trg_name\n",
    "        }\n",
    "\n",
    "# --- IMAGE PREPROCESSING ---\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "# UNFREEZE ONLY THE LAST num_last_blocks LAYERS AND THE FINAL LAYER NORM\n",
    "\n",
    "def setup_light_finetuning(model, num_last_blocks):\n",
    "    blocks_to_unfreeze = model.blocks[-num_last_blocks:]\n",
    "    N_Params = 0\n",
    "    N_Free_Params = 0\n",
    "\n",
    "    for param in model.parameters():                # FREEZE ALL\n",
    "        N_Params += param.numel()\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for block in blocks_to_unfreeze:\n",
    "\n",
    "        for param in block.parameters():\n",
    "            N_Free_Params += param.numel()           # UNFREEZE\n",
    "            param.requires_grad = True\n",
    "\n",
    "    for param in model.norm.parameters():          # UNFREEZE NORM\n",
    "        N_Free_Params += param.numel()\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # NUMBERS    \n",
    "\n",
    "    print(\"Total parameters:\", N_Params)\n",
    "    print(\"Total trainable:\", N_Free_Params)\n",
    "    print(\"Percentage trainable:\", round(100 * N_Free_Params / N_Params, 2), \"%\")\n",
    "    return model\n",
    "\n",
    "# --- LOAD AND PREPROCESS IMAGE. EXTRACT PATCH-WISE FEATURES FROM MODEL. --- \n",
    "# --- RESHAPE AND NORMALIZE FEATURE MAPS. --- \n",
    "# --- FINALLY, RETURN THE EXTRACTED FEATURE MAP AND THE ORIGINAL DIMENSIONS. --    \n",
    "\n",
    "def get_descriptors(img_path, grad):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    (w, h) = img.size\n",
    "    input_tensor = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    with torch.set_grad_enabled(grad):\n",
    "\n",
    "        if MODEL_VERSION == \"dinov2\":\n",
    "            feats = model.get_intermediate_layers(input_tensor, n=1)[0]\n",
    "            feats = feats.reshape(1, H_PATCH, W_PATCH, feats.shape[2])\n",
    "\n",
    "        elif MODEL_VERSION == \"dinov3\":\n",
    "            x = model.forward_features(input_tensor)[\"x_norm_patchtokens\"]\n",
    "            feats = x.reshape(1, H_PATCH, W_PATCH, x.shape[-1])\n",
    "\n",
    "        elif MODEL_VERSION == \"sam\":\n",
    "            predictor.set_image(np.array(img))\n",
    "            feats = predictor.get_image_embedding()\n",
    "            feats = feats.permute(0, 2, 3, 1)\n",
    "\n",
    "    feats = F.normalize(feats, dim=-1)\n",
    "    return (feats, w, h)\n",
    "\n",
    "# --- MATCH KEYPOINTS. ---\n",
    "# --- EXTRACT FEATURE SIZE, CREATE PROBABILITY GRID FOR SOFT ARG MAX\n",
    "\n",
    "def match_keypoints(batch, grad, tau=0.05):\n",
    "    src_kps = batch[\"src_kps\"][0].numpy()\n",
    "  \n",
    "    (feat_src, sw, sh) = get_descriptors(batch[\"src_path\"][0], grad)          # GET DESCRIPTORS\n",
    "    (feat_trg, tw, th) = get_descriptors(batch[\"trg_path\"][0] , grad)\n",
    "\n",
    "    (_, Hf, Wf, D) = feat_trg.shape\n",
    "    trg_flat = feat_trg[0].reshape(Hf * Wf, D)\n",
    "    pred_kps = []\n",
    "\n",
    "    (ys, xs) = torch.meshgrid(\n",
    "                  torch.arange(Hf, device=DEVICE),\n",
    "                  torch.arange(Wf, device=DEVICE),\n",
    "                  indexing=\"ij\")\n",
    "\n",
    "    for i in range(src_kps.shape[0]):\n",
    "        sx = int(src_kps[i, 0] * Wf / sw)\n",
    "        sy = int(src_kps[i, 1] * Hf / sh)\n",
    "        sx = torch.clamp(torch.tensor(sx, device=DEVICE), 0, Wf - 1)\n",
    "        sy = torch.clamp(torch.tensor(sy, device=DEVICE), 0, Hf - 1)\n",
    "        src_desc = feat_src[0, sy, sx, :]\n",
    "\n",
    "        sim = torch.matmul(trg_flat, src_desc)            # COSINE SIMILARITY\n",
    "\n",
    "        if TUNING:\n",
    "            coords = torch.stack([xs.flatten(), ys.flatten()], dim=1).float()\n",
    "\n",
    "            prob = F.softmax(sim / tau, dim=0)\n",
    "            pred_patch = (coords * prob[:, None]).sum(dim=0)\n",
    "            (px, py) = (pred_patch[0], pred_patch[1])\n",
    "\n",
    "        else:\n",
    "            best_idx = sim.argmax()\n",
    "            px = best_idx % Wf                      # ONLY INFERENCE\n",
    "            py = best_idx // Wf\n",
    "        \n",
    "        pred_x = (px + 0.5) * (tw / Wf)\n",
    "        pred_y = (py + 0.5) * (th / Hf)\n",
    "        pred_kps.append(torch.stack([pred_x, pred_y]))\n",
    "\n",
    "    return torch.stack(pred_kps)\n",
    "\n",
    "# --- VISUALIZE RESULTS AND COMPARE CORRECT AND PREDICTED KEYPOINTS ON TARGET IMAGE. ---    \n",
    "\n",
    "def visualize_keypoints(src_path, trg_path, src_kps, pred_kps, trg_kps):\n",
    "    src_img = cv2.imread(src_path)[:, :, ::-1]\n",
    "    trg_img = cv2.imread(trg_path)[:, :, ::-1]\n",
    "\n",
    "    (_, axes) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    axes[0].imshow(src_img)\n",
    "    axes[0].scatter(src_kps[:,0], src_kps[:,1], c=\"r\", s=40, label=\"src_kps\")\n",
    "    axes[0].set_title(\"Source Image\")\n",
    "\n",
    "    axes[1].imshow(trg_img)\n",
    "    axes[1].scatter(pred_kps[:,0], pred_kps[:,1], c=\"b\", s=40, label=\"pred_kps\")\n",
    "    axes[1].scatter(trg_kps[:,0], trg_kps[:,1], c=\"g\", s=40, marker=\"X\", label=\"gt_kps\")\n",
    "    axes[1].set_title(\"Target Image\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# --- TRAINING FUNCTION. ---\n",
    "\n",
    "def Train(model, number_of_Epochs, learning_rate, layers, tau=0.05):\n",
    "    global_loss = float('inf')\n",
    "    criterion = torch.nn.SmoothL1Loss()\n",
    "    dataset = SPair71kDataset()\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)           # INITIALIZATION\n",
    "    params = []\n",
    "\n",
    "    print()\n",
    "    print(\"Trying with\", layers, \"free layers\")\n",
    "    model = setup_light_finetuning(model, layers)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:                  # TRAINABLE PARAMETERS\n",
    "            params.append(p)\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "    for epoch in range(number_of_Epochs):\n",
    "        print()\n",
    "        print(\"Starting epoch\", epoch)\n",
    "        pbar = tqdm(dataloader, desc=\"Training\")\n",
    "\n",
    "        for batch in pbar:\n",
    "            trg_kps = torch.tensor(batch[\"trg_kps\"][0], device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "            pred_kps = match_keypoints(batch, grad=True, tau=tau)     # PREDICTION WITH SOFTMAX\n",
    "\n",
    "            loss = criterion(pred_kps, trg_kps)           # LOSS\n",
    "\n",
    "            optimizer.zero_grad()           # UPDATING\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # UPDATING BAR AND GLOBAL LOSS\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        if loss < global_loss:\n",
    "            path = os.path.join(PATH_EXPORT, \"best_model.pth\")          # UPDATE ON DRIVE\n",
    "            torch.save(model.state_dict(), path)\n",
    "            global_loss = loss\n",
    "\n",
    "    print(\"Training finished\")\n",
    "    return\n",
    "\n",
    "# --- EVALUATION FUNCTION ---\n",
    "# --- CONSIDER ONLY ONE CATEGORY. --- \n",
    "# --- FOR EACH PAIR: LOAD SOURCE AND TARGET IMAGES ---\n",
    "# --- EXTRACT DESCRIPTORS AND CORRECT KEYPOINTS AND RESCALE COORDINATES --- \n",
    "# --- FOR EACH ALPHA: USE COSINE SIMILARITY IN ORDER TO FIND THE CORRESPONDING POINT IN THE TARGET IMAGE. --- \n",
    "# --- FINALLY, RETURN THE CORRECT GENERATED KEYPOINTS (USING THEIR DISTANCE FROM THE ORIGINAL ONE) RATIO. ---\n",
    "# --- OPTIONALLY: VISUALIZE RESULTS. ---\n",
    "\n",
    "def run_evaluation(loader, category, visualize=False):\n",
    "    total_correct = {alpha: 0 for alpha in ALPHA}\n",
    "    total_points = 0\n",
    "    pbar = tqdm(loader, desc=\"Evaluating \" + category)\n",
    "\n",
    "    for batch in pbar:\n",
    "        src_kps = batch[\"src_kps\"][0].numpy()\n",
    "        trg_kps = batch[\"trg_kps\"][0].numpy()\n",
    "\n",
    "        pred_kps = match_keypoints(batch, False).numpy()\n",
    "\n",
    "        max_dim = max(batch[\"trg_bndbox\"][0][2]-batch[\"trg_bndbox\"][0][0], batch[\"trg_bndbox\"][0][3]-batch[\"trg_bndbox\"][0][1])\n",
    "        total_correct_image = {alpha: 0 for alpha in ALPHA}\n",
    "        total_points_image = 0                                      # FOR THE CURRENT IMAGE\n",
    "\n",
    "        for i in range(len(src_kps)):\n",
    "            dist = np.linalg.norm(pred_kps[i] - trg_kps[i])\n",
    "            total_points += 1\n",
    "            total_points_image += 1\n",
    "\n",
    "            for alpha in ALPHA:\n",
    "                if dist <= alpha * max_dim:                     # PREDICTION IS CORRECT?\n",
    "                    total_correct[alpha] += 1\n",
    "                    total_correct_image[alpha] += 1\n",
    "\n",
    "        # PRINT PER IMAGE\n",
    "\n",
    "        print()\n",
    "        print(\"Results for (\", batch[\"src_name\"][0], \",\", batch[\"trg_name\"][0], \")\")\n",
    "\n",
    "        for alpha in ALPHA:\n",
    "            total_correct_image[alpha] = round(100 * total_correct_image[alpha] / total_points_image, 2)\n",
    "            print(\"PCK@\", str(alpha), \": \", str(total_correct_image[alpha]), \"%\")\n",
    "\n",
    "        mean_pck_image = round(sum(total_correct_image.values()) / len(ALPHA), 2)\n",
    "        print(\"MEAN PCK:\", mean_pck_image, \"%\")\n",
    "        total_correct_image[\"MEAN\"] = mean_pck_image\n",
    "\n",
    "        # SAVE JSON\n",
    "\n",
    "        path = os.path.join(PATH_RES, \n",
    "                            \"pck_results_for_\" + MODEL_VERSION + \"_\" + batch[\"src_name\"][0] + \"_\" + batch[\"trg_name\"][0] + \".json\")\n",
    "        file = open(path, \"w\")\n",
    "        json.dump(total_correct_image, file, indent=4)\n",
    "        file.close()\n",
    "\n",
    "        # VISUALIZE\n",
    "\n",
    "        if visualize:\n",
    "            visualize_keypoints(batch[\"src_path\"][0], batch[\"trg_path\"][0], src_kps, pred_kps, trg_kps)\n",
    "\n",
    "    # CATEGORY PCK\n",
    "\n",
    "    print()\n",
    "    print(\"=\"*40)\n",
    "    print(\"Category:\", category)\n",
    "\n",
    "    for alpha in ALPHA:\n",
    "        total_correct[alpha] = round(100 * total_correct[alpha] / total_points, 2)\n",
    "        print(\"PCK@\", str(alpha), \": \", str(total_correct[alpha]), \"%\")\n",
    "\n",
    "    mean_pck = round(sum(total_correct.values()) / len(ALPHA), 2)\n",
    "    total_correct[\"MEAN\"] = mean_pck\n",
    "    print(\"MEAN PCK:\", mean_pck, \"%\")\n",
    "\n",
    "    path = os.path.join(PATH_RES, \"pck_results_for_\" + MODEL_VERSION + \"_\" + category + \".json\")\n",
    "    file = open(path, \"w\")\n",
    "    json.dump(total_correct, file, indent=4)\n",
    "    file.close()\n",
    "    return\n",
    "\n",
    "# --- MOUNT DRIVE AND EXTRACT DATASET ---\n",
    "\n",
    "drive.mount(PATH_DRIVE, force_remount=True)\n",
    "!tar -xzf {PATH_FILE}\n",
    "\n",
    "# --- LOAD MODEL ---\n",
    "\n",
    "print()\n",
    "print(\"Loading \", MODEL_VERSION, \" model...\")\n",
    "\n",
    "if MODEL_VERSION == \"dinov2\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitb14_reg\", pretrained=True).to(DEVICE)\n",
    "elif MODEL_VERSION == \"dinov3\":\n",
    "    model = torch.hub.load(\"facebookresearch/dinov3\", \"dinov3_vitb16\", weights=PTH_PATH).to(DEVICE)\n",
    "elif MODEL_VERSION == \"sam\":\n",
    "    model = sam_model_registry[\"vit_b\"](checkpoint=PTH_PATH).to(DEVICE)\n",
    "    predictor = SamPredictor(model)\n",
    "\n",
    "# --- TRAINING IF ENABLED ---\n",
    "\n",
    "if TUNING:\n",
    "    model.train()\n",
    "    Train(model, number_of_Epochs=5, learning_rate=1e-5, layers=3)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# --- EVALUATION PER CATEGORY ---\n",
    "\n",
    "for cat in CATEGORIES:\n",
    "    print()\n",
    "    print(\"Evaluating category: \", cat)\n",
    "\n",
    "    dataset_cat = SPair71kDataset(category_filter=cat)\n",
    "    loader = DataLoader(dataset_cat, batch_size=1, shuffle=False)\n",
    "    run_evaluation(loader, cat, visualize=False)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
