{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6dd1ba21",
      "metadata": {
        "id": "6dd1ba21"
      },
      "source": [
        "# DINOv2 ViT-B_reg Configuration for Light Fine-tuning\n",
        "\n",
        "This notebook demonstrates how to load and configure the DINOv3 ViT-Base model with registration tokens (ViT-B_reg) for light fine-tuning of the last layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "49ee2f76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49ee2f76",
        "outputId": "6a440dc4-9a2f-4b1b-ca0f-6f44eca7a925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_reg4_pretrain.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 330M/330M [00:00<00:00, 442MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total transformer blocks: 12\n",
            "‚úì Unfrozen block 9\n",
            "‚úì Unfrozen block 10\n",
            "‚úì Unfrozen block 11\n",
            "‚úì Unfrozen final layer norm\n",
            "\n",
            "After light fine-tuning setup:\n",
            "Total parameters: 86,583,552\n",
            "Trainable parameters: 21,269,760\n",
            "Percentage trainable: 24.57%\n",
            "üì• Downloading shared data...\n",
            "Retrieving folder contents\n",
            "Retrieving folder 1TB7W0mx4rhLShdsyGNnT3BvrDAD-vCE6 results\n",
            "Retrieving folder 18UvZK8jzUhku4YasNUk9c0avpXJZww7I metrics\n",
            "Retrieving folder 1ftqXNTeL6lWLi_cAakQXLtX-bvtidaVq plots\n",
            "Retrieving folder 1zWFUZxXVqrLemn-ZWn5jh-frtA4Z1GWM weights\n",
            "Processing file 1lvmW5fGM_O2DNbb9eEtKvmh69_TSBxqM dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\n",
            "Processing file 1j0wXloH_YepR9w2xkXQOcVm4Zy8QVzQR sam_vit_b_01ec64.pth\n",
            "Processing file 14WscxapP53HYX9JOd9euD5wqJtBqIFmP DINOv2_eval.ipynb\n",
            "Processing file 1Awnsme0UfwOwevbKPT3SXW-FL-Hjlopg SPair-71k.tar.gz\n",
            "Processing file 1j32zxdYhEMmEfq9DoTA3gpyVUz3yHZgR Steps.ipynb\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1lvmW5fGM_O2DNbb9eEtKvmh69_TSBxqM\n",
            "From (redirected): https://drive.google.com/uc?id=1lvmW5fGM_O2DNbb9eEtKvmh69_TSBxqM&confirm=t&uuid=42f15b23-cda3-4c31-a52d-2096946ebccf\n",
            "To: /content/shared_data/weights/dinov3_vitb16_pretrain_lvd1689m-73cec8be.pth\n",
            "100% 343M/343M [00:02<00:00, 126MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1j0wXloH_YepR9w2xkXQOcVm4Zy8QVzQR\n",
            "From (redirected): https://drive.google.com/uc?id=1j0wXloH_YepR9w2xkXQOcVm4Zy8QVzQR&confirm=t&uuid=7db62ee3-0434-48ef-8654-513bcfb10cd6\n",
            "To: /content/shared_data/weights/sam_vit_b_01ec64.pth\n",
            "100% 375M/375M [00:04<00:00, 91.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14WscxapP53HYX9JOd9euD5wqJtBqIFmP\n",
            "To: /content/shared_data/DINOv2_eval.ipynb\n",
            "100% 8.50k/8.50k [00:00<00:00, 32.2MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Awnsme0UfwOwevbKPT3SXW-FL-Hjlopg\n",
            "From (redirected): https://drive.google.com/uc?id=1Awnsme0UfwOwevbKPT3SXW-FL-Hjlopg&confirm=t&uuid=2df8fff7-e69d-45a1-9686-bf0d8d048f93\n",
            "To: /content/shared_data/SPair-71k.tar.gz\n",
            "100% 227M/227M [00:01<00:00, 185MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j32zxdYhEMmEfq9DoTA3gpyVUz3yHZgR\n",
            "To: /content/shared_data/Steps.ipynb\n",
            "100% 41.6k/41.6k [00:00<00:00, 97.3MB/s]\n",
            "Download completed\n",
            "\n",
            "üîó Setting up dataset from shared data...\n",
            "Found /content/shared_data/SPair-71k.tar.gz, extracting...\n",
            "‚úÖ SPair-71k.tar.gz extracted.\n",
            "‚úÖ SPair-71k dataset linked from shared data\n",
            "‚úÖ Dataset verification successful\n",
            "lrwxrwxrwx 1 root root 30 Dec 22 02:09 /content/semantic-correspondence/data/SPair-71k -> /content/shared_data/SPair-71k\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Load DINOv2 ViT-B/14 with registration tokens via torch.hub\n",
        "dinov2_vitb14_reg = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14_reg', pretrained=True)\n",
        "\n",
        "\n",
        "def setup_light_finetuning(model, num_last_blocks=3):\n",
        "    \"\"\"\n",
        "    Setup light fine-tuning by unfreezing only the last transformer blocks\n",
        "\n",
        "    Args:\n",
        "        model: DINOv2 model\n",
        "        num_last_blocks: Number of last blocks to unfreeze (recommended: 2-4)\n",
        "    \"\"\"\n",
        "    if model is None:\n",
        "        print(\"Model not available\")\n",
        "        return None\n",
        "\n",
        "    # First, freeze all parameters\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Get total number of blocks\n",
        "    total_blocks = len(model.blocks)\n",
        "    print(f\"Total transformer blocks: {total_blocks}\")\n",
        "\n",
        "    # Unfreeze the last num_last_blocks blocks\n",
        "    blocks_to_unfreeze = list(range(total_blocks - num_last_blocks, total_blocks))\n",
        "\n",
        "    for block_idx in blocks_to_unfreeze:\n",
        "        for param in model.blocks[block_idx].parameters():\n",
        "            param.requires_grad = True\n",
        "        print(f\"‚úì Unfrozen block {block_idx}\")\n",
        "\n",
        "    # Also unfreeze the final layer norm\n",
        "    for param in model.norm.parameters():\n",
        "        param.requires_grad = True\n",
        "    print(\"‚úì Unfrozen final layer norm\")\n",
        "\n",
        "    # Count trainable parameters after setup\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nAfter light fine-tuning setup:\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Percentage trainable: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Apply light fine-tuning configuration\n",
        "dinov2_vitb14_reg = setup_light_finetuning(dinov2_vitb14_reg, num_last_blocks=3)\n",
        "\n",
        "# Dataset loading\n",
        "\n",
        "SHARED_FOLDER_ID = \"1O9YEQGv9j-4DGQp6adu5zbo3UH9K4ohJ\"\n",
        "\n",
        "# Download shared data from Google Drive\n",
        "print(\"üì• Downloading shared data...\")\n",
        "!pip install -q gdown\n",
        "!gdown --folder https://drive.google.com/drive/folders/1O9YEQGv9j-4DGQp6adu5zbo3UH9K4ohJ -O /content/shared_data\n",
        "\n",
        "\n",
        "# 3. Link dataset from shared data\n",
        "print(\"\\nüîó Setting up dataset from shared data...\")\n",
        "!mkdir -p /content/semantic-correspondence/data\n",
        "\n",
        "# Extract SPair-71k.tar.gz if it exists, then link\n",
        "SPAIR_TAR_PATH = '/content/shared_data/SPair-71k.tar.gz'\n",
        "SPAIR_DIR_PATH = '/content/shared_data/SPair-71k'\n",
        "\n",
        "if os.path.exists(SPAIR_TAR_PATH):\n",
        "    print(f\"Found {SPAIR_TAR_PATH}, extracting...\")\n",
        "    !tar -xzf {SPAIR_TAR_PATH} -C /content/shared_data\n",
        "    print(\"‚úÖ SPair-71k.tar.gz extracted.\")\n",
        "\n",
        "# Link SPair-71k directly from shared data\n",
        "if os.path.exists(SPAIR_DIR_PATH):\n",
        "    !ln -sf {SPAIR_DIR_PATH} /content/semantic-correspondence/data/SPair-71k\n",
        "    print(\"‚úÖ SPair-71k dataset linked from shared data\")\n",
        "else:\n",
        "    print(\"‚ùå SPair-71k not found in shared data\")\n",
        "    print(\"Available folders:\")\n",
        "    !ls -la /content/shared_data\n",
        "\n",
        "# Verify dataset setup\n",
        "if os.path.exists('/content/semantic-correspondence/data/SPair-71k'):\n",
        "    print(\"‚úÖ Dataset verification successful\")\n",
        "    !ls -la /content/semantic-correspondence/data/SPair-71k | head -5\n",
        "else:\n",
        "    print(\"‚ùå Dataset link verification failed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b32a409",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1b32a409",
        "outputId": "5364fbee-f5f6-4471-af47-4dc9b5fac060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "‚úì Loss function initialized\n",
            "Trainable parameter: blocks.9.norm1.weight, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.9.norm1.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.9.attn.qkv.weight, shape: torch.Size([2304, 768])\n",
            "Trainable parameter: blocks.9.attn.qkv.bias, shape: torch.Size([2304])\n",
            "Trainable parameter: blocks.9.attn.proj.weight, shape: torch.Size([768, 768])\n",
            "Trainable parameter: blocks.9.attn.proj.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.9.ls1.gamma, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.9.norm2.weight, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.9.norm2.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.9.mlp.fc1.weight, shape: torch.Size([3072, 768])\n",
            "Trainable parameter: blocks.9.mlp.fc1.bias, shape: torch.Size([3072])\n",
            "Trainable parameter: blocks.9.mlp.fc2.weight, shape: torch.Size([768, 3072])\n",
            "Trainable parameter: blocks.9.mlp.fc2.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.9.ls2.gamma, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.norm1.weight, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.norm1.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.attn.qkv.weight, shape: torch.Size([2304, 768])\n",
            "Trainable parameter: blocks.10.attn.qkv.bias, shape: torch.Size([2304])\n",
            "Trainable parameter: blocks.10.attn.proj.weight, shape: torch.Size([768, 768])\n",
            "Trainable parameter: blocks.10.attn.proj.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.ls1.gamma, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.norm2.weight, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.norm2.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.mlp.fc1.weight, shape: torch.Size([3072, 768])\n",
            "Trainable parameter: blocks.10.mlp.fc1.bias, shape: torch.Size([3072])\n",
            "Trainable parameter: blocks.10.mlp.fc2.weight, shape: torch.Size([768, 3072])\n",
            "Trainable parameter: blocks.10.mlp.fc2.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.10.ls2.gamma, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.norm1.weight, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.norm1.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.attn.qkv.weight, shape: torch.Size([2304, 768])\n",
            "Trainable parameter: blocks.11.attn.qkv.bias, shape: torch.Size([2304])\n",
            "Trainable parameter: blocks.11.attn.proj.weight, shape: torch.Size([768, 768])\n",
            "Trainable parameter: blocks.11.attn.proj.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.ls1.gamma, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.norm2.weight, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.norm2.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.mlp.fc1.weight, shape: torch.Size([3072, 768])\n",
            "Trainable parameter: blocks.11.mlp.fc1.bias, shape: torch.Size([3072])\n",
            "Trainable parameter: blocks.11.mlp.fc2.weight, shape: torch.Size([768, 3072])\n",
            "Trainable parameter: blocks.11.mlp.fc2.bias, shape: torch.Size([768])\n",
            "Trainable parameter: blocks.11.ls2.gamma, shape: torch.Size([768])\n",
            "Trainable parameter: norm.weight, shape: torch.Size([768])\n",
            "Trainable parameter: norm.bias, shape: torch.Size([768])\n",
            "\n",
            "Total trainable parameters: 21,269,760\n",
            "‚úì Optimizer configured: Adam with LR=0.0001\n",
            "‚úì Scheduler configured: OneCycleLR with 1000 total steps\n",
            "‚úì Model moved to device: cuda\n",
            "Loaded 53340 pairs for train split\n",
            "Loaded 5384 pairs for val split\n",
            "Starting training...\n",
            "\n",
            "==================================================\n",
            "Epoch 1/10\n",
            "==================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:  11%|‚ñà         | 723/6668 [04:37<37:59,  2.61it/s, Loss=0.1287, LR=2.15e-05]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1823855495.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m     train_metrics = train_one_epoch(\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdinov2_vitb14_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1823855495.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, scheduler, scaler, epoch)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;31m# Update metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0mtotal_corr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'correspondence_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mtotal_consistency_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'consistency_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset # Added Dataset import\n",
        "import sys\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import json\n",
        "\n",
        "# Set device at the beginning of this cell to ensure it's always defined\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Training hyperparameters\n",
        "LEARNING_RATE = 1e-4  # Lower LR for fine-tuning\n",
        "BATCH_SIZE = 8        # Adjust based on GPU memory\n",
        "NUM_EPOCHS = 10\n",
        "WARMUP_EPOCHS = 1\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "class SemanticCorrespondenceLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Comprehensive loss function for semantic correspondence training\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 lambda_correspondence=1.0,\n",
        "                 lambda_consistency=0.5,\n",
        "                 lambda_smooth=0.1,\n",
        "                 temperature=0.1):\n",
        "        super().__init__()\n",
        "        self.lambda_correspondence = lambda_correspondence\n",
        "        self.lambda_consistency = lambda_consistency\n",
        "        self.lambda_smooth = lambda_smooth\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def correspondence_loss(self, feat1, feat2, kpts1, kpts2, valid_mask):\n",
        "        \"\"\"\n",
        "        Keypoint correspondence loss using feature similarity\n",
        "        \"\"\"\n",
        "        # Normalize features\n",
        "        feat1 = F.normalize(feat1, dim=-1)\n",
        "        feat2 = F.normalize(feat2, dim=-1)\n",
        "\n",
        "        # Compute similarity matrix\n",
        "        similarity = torch.matmul(feat1, feat2.transpose(-2, -1)) / self.temperature\n",
        "\n",
        "        # Get ground truth correspondence indices\n",
        "        batch_size = feat1.size(0)\n",
        "        losses = []\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            if valid_mask[b].sum() == 0:\n",
        "                continue\n",
        "\n",
        "            # Get valid keypoints for this batch\n",
        "            valid_kpts1 = kpts1[b][valid_mask[b]]  # [N_valid, 2]\n",
        "            valid_kpts2 = kpts2[b][valid_mask[b]]  # [N_valid, 2]\n",
        "\n",
        "            if len(valid_kpts1) == 0:\n",
        "                continue\n",
        "\n",
        "            # Convert keypoints to feature indices (assuming features are spatial)\n",
        "            H, W = int(feat1.size(1) ** 0.5), int(feat1.size(1) ** 0.5)\n",
        "\n",
        "            # Get feature similarities for valid correspondences\n",
        "            sim_batch = similarity[b]  # [H*W, H*W]\n",
        "\n",
        "            # Create correspondence targets (identity for matched keypoints)\n",
        "            n_valid = len(valid_kpts1)\n",
        "            targets = torch.arange(n_valid, device=feat1.device)\n",
        "\n",
        "            # Select relevant similarities\n",
        "            if n_valid > 1:\n",
        "                selected_sim = sim_batch[:n_valid, :n_valid]\n",
        "                loss_batch = F.cross_entropy(selected_sim, targets)\n",
        "                losses.append(loss_batch)\n",
        "\n",
        "        if len(losses) == 0:\n",
        "            return torch.tensor(0.0, device=feat1.device, requires_grad=True)\n",
        "\n",
        "        return torch.stack(losses).mean()\n",
        "\n",
        "    def cycle_consistency_loss(self, feat1, feat2):\n",
        "        \"\"\"\n",
        "        Enforce cycle consistency: feat1 -> feat2 -> feat1\n",
        "        \"\"\"\n",
        "        # Normalize features\n",
        "        feat1_norm = F.normalize(feat1, dim=-1)\n",
        "        feat2_norm = F.normalize(feat2, dim=-1)\n",
        "\n",
        "        # Forward correspondence: feat1 -> feat2\n",
        "        sim_12 = torch.matmul(feat1_norm, feat2_norm.transpose(-2, -1))\n",
        "        correspondence_12 = F.softmax(sim_12 / self.temperature, dim=-1)\n",
        "\n",
        "        # Backward correspondence: feat2 -> feat1\n",
        "        sim_21 = torch.matmul(feat2_norm, feat1_norm.transpose(-2, -1))\n",
        "        correspondence_21 = F.softmax(sim_21 / self.temperature, dim=-1)\n",
        "\n",
        "        # Cycle consistency: should get back to identity\n",
        "        cycle_consistency = torch.matmul(correspondence_12, correspondence_21)\n",
        "        identity = torch.eye(feat1.size(1), device=feat1.device).unsqueeze(0).expand(feat1.size(0), -1, -1)\n",
        "\n",
        "        return F.mse_loss(cycle_consistency, identity)\n",
        "\n",
        "    def smoothness_loss(self, feat1, feat2):\n",
        "        \"\"\"\n",
        "        Encourage smooth correspondences in spatial neighborhoods\n",
        "        \"\"\"\n",
        "        # Simple spatial smoothness - encourage similar features for nearby patches\n",
        "        B, N, D = feat1.shape\n",
        "        H = W = int(N ** 0.5)\n",
        "\n",
        "        if H * W != N:\n",
        "            return torch.tensor(0.0, device=feat1.device, requires_grad=True)\n",
        "\n",
        "        # Reshape to spatial dimensions\n",
        "        feat1_spatial = feat1.view(B, H, W, D)\n",
        "        feat2_spatial = feat2.view(B, H, W, D)\n",
        "\n",
        "        # Compute differences with neighbors (4-connectivity)\n",
        "        smooth_loss = 0.0\n",
        "        count = 0\n",
        "\n",
        "        for dx, dy in [(0, 1), (1, 0), (0, -1), (-1, 0)]:\n",
        "            # The original condition `0 <= dx < H-1 and 0 <= dy < W-1` might be too restrictive.\n",
        "            # It should be `0 <= x+dx < H` and `0 <= y+dy < W` when iterating over elements.\n",
        "            # For simpler tensor slicing, ensure the dimensions are valid for `dx` and `dy`.\n",
        "            if dx == 0 and dy == 1: # Right neighbor\n",
        "                diff1 = feat1_spatial[:, :, :-1] - feat1_spatial[:, :, 1:]\n",
        "                diff2 = feat2_spatial[:, :, :-1] - feat2_spatial[:, :, 1:]\n",
        "                smooth_loss += F.mse_loss(diff1, diff2)\n",
        "                count += 1\n",
        "            elif dx == 1 and dy == 0: # Down neighbor\n",
        "                diff1 = feat1_spatial[:, :-1, :] - feat1_spatial[:, 1:, :]\n",
        "                diff2 = feat2_spatial[:, :-1, :] - feat2_spatial[:, 1:, :]\n",
        "                smooth_loss += F.mse_loss(diff1, diff2)\n",
        "                count += 1\n",
        "            # For (0, -1) and (-1, 0), it's more complex to implement with direct slicing like this\n",
        "            # and usually requires padding or careful boundary handling if using naive subtraction.\n",
        "            # Given this is 'simple spatial smoothness', sticking to right and down neighbors for now.\n",
        "\n",
        "        return smooth_loss / max(count, 1)\n",
        "\n",
        "    def forward(self, feat1, feat2, kpts1, kpts2, valid_mask):\n",
        "        \"\"\"\n",
        "        Compute total loss\n",
        "        \"\"\"\n",
        "        # Main correspondence loss\n",
        "        corr_loss = self.correspondence_loss(feat1, feat2, kpts1, kpts2, valid_mask)\n",
        "        #print(f\"[DEBUG Loss] correspondence_loss: {corr_loss}\")\n",
        "\n",
        "        # Cycle consistency loss\n",
        "        consistency_loss = self.cycle_consistency_loss(feat1, feat2)\n",
        "        #print(f\"[DEBUG Loss] consistency_loss: {consistency_loss}\")\n",
        "\n",
        "        # Smoothness loss\n",
        "        smooth_loss = self.smoothness_loss(feat1, feat2)\n",
        "        #print(f\"[DEBUG Loss] smoothness_loss: {smooth_loss}\")\n",
        "\n",
        "        # Total loss\n",
        "        total_loss = (self.lambda_correspondence * corr_loss +\n",
        "                     self.lambda_consistency * consistency_loss +\n",
        "                     self.lambda_smooth * smooth_loss)\n",
        "        #print(f\"[DEBUG Loss] total_loss: {total_loss}\")\n",
        "\n",
        "        return {\n",
        "            'total_loss': total_loss,\n",
        "            'correspondence_loss': corr_loss,\n",
        "            'consistency_loss': consistency_loss,\n",
        "            'smoothness_loss': smooth_loss\n",
        "        }\n",
        "\n",
        "# Initialize loss function\n",
        "criterion = SemanticCorrespondenceLoss(\n",
        "    lambda_correspondence=1.0,\n",
        "    lambda_consistency=0.5,\n",
        "    lambda_smooth=0.1,\n",
        "    temperature=0.1\n",
        ")\n",
        "print(\"‚úì Loss function initialized\")\n",
        "\n",
        "# Create parameter groups with different learning rates\n",
        "trainable_params = []\n",
        "for name, param in dinov2_vitb14_reg.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        trainable_params.append(param)\n",
        "        print(f\"Trainable parameter: {name}, shape: {param.shape}\")\n",
        "\n",
        "print(f\"\\nTotal trainable parameters: {sum(p.numel() for p in trainable_params):,}\")\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer = torch.optim.Adam(\n",
        "    trainable_params,\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY,\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "print(f\"‚úì Optimizer configured: Adam with LR={LEARNING_RATE}\")\n",
        "\n",
        "# Move model to device and ensure proper setup\n",
        "dinov2_vitb14_reg = dinov2_vitb14_reg.to(device)\n",
        "dinov2_vitb14_reg.train()\n",
        "print(f\"‚úì Model moved to device: {device}\")\n",
        "\n",
        "def extract_features(model, images, layer_idx=-1):\n",
        "    \"\"\"\n",
        "    Extract features from DINOv2 model\n",
        "\n",
        "    Args:\n",
        "        model: DINOv2 model\n",
        "        images: Input images [B, 3, H, W]\n",
        "        layer_idx: Which layer to extract features from (-1 for last layer)\n",
        "\n",
        "    Returns:\n",
        "        features: Extracted features [B, N, D] where N is number of patches\n",
        "    \"\"\"\n",
        "    # Removed mixed precision autocast for compatibility\n",
        "    # autocast_dtype = torch.float16 if device.type == 'cuda' else torch.bfloat16\n",
        "\n",
        "    # with torch.amp.autocast(device_type=device.type, dtype=autocast_dtype):\n",
        "    if layer_idx == -1:\n",
        "        # Use get_intermediate_layers to ensure we get the full sequence of tokens from the last block\n",
        "        # For the last layer, n should be [len(model.blocks) - 1]\n",
        "        last_block_idx = len(model.blocks) - 1\n",
        "        intermediate_output = model.get_intermediate_layers(\n",
        "            images,\n",
        "            n=[last_block_idx],\n",
        "            return_class_token=False  # Set to False to get just reg + patch tokens\n",
        "        )\n",
        "\n",
        "        features = intermediate_output[0]  # This will be [B, num_reg+N_patch, D]\n",
        "\n",
        "    else:\n",
        "        # This branch already uses get_intermediate_layers correctly with return_class_token=False\n",
        "        intermediate_output = model.get_intermediate_layers(\n",
        "            images,\n",
        "            n=[layer_idx],\n",
        "            return_class_token=False\n",
        "        )\n",
        "        features = intermediate_output[0]\n",
        "        # Remove register tokens if present\n",
        "        features = features[:, model.num_register_tokens:]\n",
        "\n",
        "    return features\n",
        "\n",
        "def prepare_batch_data(batch):\n",
        "    \"\"\"\n",
        "    Prepare batch data for training\n",
        "\n",
        "    Args:\n",
        "        batch: Batch from dataloader containing images and keypoints\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prepared data\n",
        "    \"\"\"\n",
        "    # Extract data from batch (adjust according to your dataset format)\n",
        "    img1 = batch['img1'].to(device)  # [B, 3, H, W]\n",
        "    img2 = batch['img2'].to(device)  # [B, 3, H, W]\n",
        "    kpts1 = batch['kpts1'].to(device)  # [B, N, 2]\n",
        "    kpts2 = batch['kpts2'].to(device)  # [B, N, 2]\n",
        "    valid_mask = batch['valid'].to(device)  # [B, N] - which keypoints are valid\n",
        "\n",
        "    return {\n",
        "        'img1': img1,\n",
        "        'img2': img2,\n",
        "        'kpts1': kpts1,\n",
        "        'kpts2': kpts2,\n",
        "        'valid_mask': valid_mask\n",
        "    }\n",
        "\n",
        "\n",
        "class SPair71kDataset(Dataset):\n",
        "    \"\"\"\n",
        "    SPair-71k Dataset for semantic correspondence\n",
        "\n",
        "    Expected data structure:\n",
        "    SPair-71k/\n",
        "    ‚îú‚îÄ‚îÄ JPEGImages/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ cat/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ dog/\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "    ‚îú‚îÄ‚îÄ PairAnnotation/\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ trn/  # <--- Note 'trn', 'test', 'val' instead of 'train', 'val'\n",
        "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000001-img1-img2:category.json\n",
        "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ val/\n",
        "    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 000002-img3-img4:category.json\n",
        "    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ test/\n",
        "    ‚îÇ       ‚îú‚îÄ‚îÄ 000003-img5-img4:category.json\n",
        "    ‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
        "    ‚îî‚îÄ‚îÄ ImageAnnotation/\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 data_root,\n",
        "                 split='train', # 'train', 'val', 'test'\n",
        "                 image_size=224,\n",
        "                 category=None):\n",
        "        self.data_root = data_root\n",
        "        # Map 'train' to 'trn' for directory names as per dataset structure\n",
        "        self.split_dir = 'trn' if split == 'train' else split\n",
        "        self.image_size = image_size\n",
        "        self.category = category\n",
        "\n",
        "        # Image transforms\n",
        "        self.transform = T.Compose([\n",
        "            T.Resize((image_size, image_size)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        # Load annotations\n",
        "        self.pairs = self._load_pairs()\n",
        "        print(f\"Loaded {len(self.pairs)} pairs for {split} split\")\n",
        "\n",
        "    def _load_pairs(self):\n",
        "        \"\"\"Load pair annotations from SPair-71k format, with error handling for missing fields.\"\"\"\n",
        "        pairs = []\n",
        "\n",
        "        # Construct the path to the split-specific annotation directory\n",
        "        # e.g., /content/semantic-correspondence/data/SPair-71k/PairAnnotation/trn\n",
        "        split_annotation_root = os.path.join(self.data_root, 'PairAnnotation', self.split_dir)\n",
        "\n",
        "        if not os.path.exists(split_annotation_root) or not os.path.isdir(split_annotation_root):\n",
        "            print(f\"Warning: Split annotation directory not found: {split_annotation_root}. Returning empty dataset.\")\n",
        "            return []\n",
        "\n",
        "        # JSON files are directly within the split_annotation_root\n",
        "        for pair_file_name in os.listdir(split_annotation_root):\n",
        "            if not pair_file_name.endswith('.json'):\n",
        "                continue\n",
        "\n",
        "            pair_path = os.path.join(split_annotation_root, pair_file_name)\n",
        "            try:\n",
        "                with open(pair_path, 'r') as f:\n",
        "                    pair_data = json.load(f)\n",
        "\n",
        "                src_imname = pair_data.get('src_imname')\n",
        "                trg_imname = pair_data.get('trg_imname')\n",
        "                src_kps = pair_data.get('src_kps')\n",
        "                trg_kps = pair_data.get('trg_kps')\n",
        "                actual_category_from_json = pair_data.get('category')\n",
        "\n",
        "                # Handle 'valid_kpts' - not explicitly present in sample JSON, assume all true\n",
        "                valid_kpts = pair_data.get('valid_kpts')\n",
        "                if valid_kpts is None:\n",
        "                    valid_kpts = [True] * len(src_kps) if src_kps else []\n",
        "                elif not isinstance(valid_kpts, list):\n",
        "                    print(f\"Warning: 'valid_kpts' in {pair_file_name} is not a list. Defaulting to all True.\")\n",
        "                    valid_kpts = [True] * len(src_kps) if src_kps else []\n",
        "\n",
        "                # Essential check: Ensure img paths and keypoints are present and are not None\n",
        "                if not (isinstance(src_imname, str) and isinstance(trg_imname, str) and\n",
        "                        isinstance(src_kps, list) and isinstance(trg_kps, list) and\n",
        "                        isinstance(actual_category_from_json, str)):\n",
        "                    print(f\"Warning: Skipping malformed pair file {pair_file_name} due to missing or invalid essential data (src_imname:{type(src_imname)}, trg_imname:{type(trg_imname)}, src_kps:{type(src_kps)}, trg_kps:{type(trg_kps)}, category:{type(actual_category_from_json)}).\")\n",
        "                    continue\n",
        "\n",
        "                # Check for consistent lengths of keypoint lists\n",
        "                if not (len(src_kps) == len(trg_kps) and len(src_kps) == len(valid_kpts)):\n",
        "                    print(f\"Warning: Skipping pair {pair_file_name} due to mismatch in keypoint/valid_mask lengths. \"\n",
        "                          f\"src_kps:{len(src_kps)}, trg_kps:{len(trg_kps)}, valid_kpts:{len(valid_kpts)}.\")\n",
        "                    continue\n",
        "\n",
        "                # Filter by category if specified\n",
        "                if self.category and actual_category_from_json != self.category:\n",
        "                    continue\n",
        "\n",
        "                pairs.append({\n",
        "                    'category': actual_category_from_json, # Use the category from JSON\n",
        "                    'src_img': src_imname,\n",
        "                    'trg_img': trg_imname,\n",
        "                    'src_kpts': src_kps,\n",
        "                    'trg_kpts': trg_kps,\n",
        "                    'valid': valid_kpts\n",
        "                })\n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Warning: Skipping malformed JSON file {pair_file_name}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: An unexpected error occurred while processing {pair_file_name}: {e}\")\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "\n",
        "        # Load images\n",
        "        img1_path = os.path.join(self.data_root, 'JPEGImages', pair['category'], pair['src_img'])\n",
        "        img2_path = os.path.join(self.data_root, 'JPEGImages', pair['category'], pair['trg_img'])\n",
        "\n",
        "        img1 = Image.open(img1_path).convert('RGB')\n",
        "        img2 = Image.open(img2_path).convert('RGB')\n",
        "\n",
        "        # Get original dimensions for keypoint scaling\n",
        "        orig_w1, orig_h1 = img1.size\n",
        "        orig_w2, orig_h2 = img2.size\n",
        "\n",
        "        # Transform images\n",
        "        img1_tensor = self.transform(img1)\n",
        "        img2_tensor = self.transform(img2)\n",
        "\n",
        "        # Scale keypoints to match resized image\n",
        "        kpts1 = torch.tensor(pair['src_kpts'], dtype=torch.float32)\n",
        "        kpts2 = torch.tensor(pair['trg_kpts'], dtype=torch.float32)\n",
        "\n",
        "        # Scale keypoints from original size to self.image_size\n",
        "        kpts1[:, 0] = kpts1[:, 0] * self.image_size / orig_w1\n",
        "        kpts1[:, 1] = kpts1[:, 1] * self.image_size / orig_h1\n",
        "        kpts2[:, 0] = kpts2[:, 0] * self.image_size / orig_w2\n",
        "        kpts2[:, 1] = kpts2[:, 1] * self.image_size / orig_h2\n",
        "\n",
        "        # Valid mask\n",
        "        valid_mask = torch.tensor(pair['valid'], dtype=torch.bool)\n",
        "\n",
        "        return {\n",
        "            'img1': img1_tensor,\n",
        "            'img2': img2_tensor,\n",
        "            'kpts1': kpts1,\n",
        "            'kpts2': kpts2,\n",
        "            'valid': valid_mask,\n",
        "            'category': pair['category']\n",
        "        }\n",
        "\n",
        "# Custom collate_fn for handling varying number of keypoints\n",
        "def custom_collate_fn(batch):\n",
        "    imgs1 = torch.stack([item['img1'] for item in batch])\n",
        "    imgs2 = torch.stack([item['img2'] for item in batch])\n",
        "\n",
        "    # Find the maximum number of keypoints in the current batch\n",
        "    max_kpts = max([item['kpts1'].shape[0] for item in batch])\n",
        "\n",
        "    padded_kpts1 = []\n",
        "    padded_kpts2 = []\n",
        "    padded_valid_mask = []\n",
        "\n",
        "    for item in batch:\n",
        "        num_kpts = item['kpts1'].shape[0]\n",
        "        # Pad kpts1 to max_kpts\n",
        "        pad_kpts1 = F.pad(item['kpts1'], (0, 0, 0, max_kpts - num_kpts), 'constant', 0)\n",
        "        padded_kpts1.append(pad_kpts1)\n",
        "\n",
        "        # Pad kpts2 to max_kpts\n",
        "        pad_kpts2 = F.pad(item['kpts2'], (0, 0, 0, max_kpts - num_kpts), 'constant', 0)\n",
        "        padded_kpts2.append(pad_kpts2)\n",
        "\n",
        "        # Pad valid mask to max_kpts (fill with False for padded values)\n",
        "        pad_valid_mask = F.pad(item['valid'], (0, max_kpts - num_kpts), 'constant', False)\n",
        "        padded_valid_mask.append(pad_valid_mask)\n",
        "\n",
        "    kpts1_batch = torch.stack(padded_kpts1)\n",
        "    kpts2_batch = torch.stack(padded_kpts2)\n",
        "    valid_mask_batch = torch.stack(padded_valid_mask)\n",
        "\n",
        "    categories = [item['category'] for item in batch]\n",
        "\n",
        "    return {\n",
        "        'img1': imgs1,\n",
        "        'img2': imgs2,\n",
        "        'kpts1': kpts1_batch,\n",
        "        'kpts2': kpts2_batch,\n",
        "        'valid': valid_mask_batch,\n",
        "        'category': categories\n",
        "    }\n",
        "\n",
        "# Define training and validation functions\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, scheduler, scaler, epoch):\n",
        "    \"\"\"\n",
        "    Train for one epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_corr_loss = 0\n",
        "    total_consistency_loss = 0\n",
        "    total_smooth_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "\n",
        "    # Disable mixed precision to fix type mismatch issues\n",
        "    # autocast_dtype = torch.float16 if device.type == 'cuda' else torch.bfloat16\n",
        "\n",
        "    for batch_idx, batch in enumerate(progress_bar):\n",
        "        try:\n",
        "            # Prepare batch data\n",
        "            data = prepare_batch_data(batch)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Standard forward pass without autocast to avoid type mismatches\n",
        "            # with torch.amp.autocast(device_type=device.type, dtype=autocast_dtype):\n",
        "            # Extract features\n",
        "            feat1 = extract_features(model, data['img1'])\n",
        "            feat2 = extract_features(model, data['img2'])\n",
        "\n",
        "            # Compute loss\n",
        "            loss_dict = criterion(\n",
        "                feat1, feat2,\n",
        "                data['kpts1'], data['kpts2'],\n",
        "                data['valid_mask']\n",
        "            )\n",
        "\n",
        "            loss = loss_dict['total_loss']\n",
        "\n",
        "            # Standard backward pass without gradient scaling\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Update metrics\n",
        "            total_loss += loss.item()\n",
        "            total_corr_loss += loss_dict['correspondence_loss'].item()\n",
        "            total_consistency_loss += loss_dict['consistency_loss'].item()\n",
        "            total_smooth_loss += loss_dict['smoothness_loss'].item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'LR': f'{current_lr:.2e}'\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch {batch_idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_loss = total_loss / max(num_batches, 1)\n",
        "    avg_corr_loss = total_corr_loss / max(num_batches, 1)\n",
        "    avg_consistency_loss = total_consistency_loss / max(num_batches, 1)\n",
        "    avg_smooth_loss = total_smooth_loss / max(num_batches, 1)\n",
        "\n",
        "    return {\n",
        "        'total_loss': avg_loss,\n",
        "        'correspondence_loss': avg_corr_loss,\n",
        "        'consistency_loss': avg_consistency_loss,\n",
        "        'smoothness_loss': avg_smooth_loss\n",
        "    }\n",
        "\n",
        "def validate_model(model, dataloader, criterion):\n",
        "    \"\"\"\n",
        "    Validate the model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    # Disable mixed precision for validation as well\n",
        "    # autocast_dtype = torch.float16 if device.type == 'cuda' else torch.bfloat16\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validation\"):\n",
        "            try:\n",
        "                data = prepare_batch_data(batch)\n",
        "\n",
        "                # Standard forward pass without autocast\n",
        "                # with torch.amp.autocast(device_type=device.type, dtype=autocast_dtype):\n",
        "                # Extract features\n",
        "                feat1 = extract_features(model, data['img1'])\n",
        "                feat2 = extract_features(model, data['img2'])\n",
        "\n",
        "                loss_dict = criterion(\n",
        "                    feat1, feat2,\n",
        "                    data['kpts1'], data['kpts2'],\n",
        "                    data['valid_mask']\n",
        "                )\n",
        "\n",
        "                loss = loss_dict['total_loss']\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Validation error: {e}\")\n",
        "                continue\n",
        "\n",
        "    return total_loss / max(num_batches, 1)\n",
        "\n",
        "# Initialize mixed precision scaler using torch.amp.GradScaler (disabled for compatibility)\n",
        "# scaler = torch.amp.GradScaler()\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Create datasets using downloaded data\n",
        "train_dataset = SPair71kDataset(\n",
        "    data_root='/content/semantic-correspondence/data/SPair-71k',\n",
        "    split='train',\n",
        "    image_size=224\n",
        ")\n",
        "\n",
        "val_dataset = SPair71kDataset(\n",
        "    data_root='/content/semantic-correspondence/data/SPair-71k',\n",
        "    split='val',\n",
        "    image_size=224\n",
        ")\n",
        "\n",
        "# Create dataloaders, now using the custom_collate_fn\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=custom_collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Learning rate scheduler - now we can calculate correct total_steps\n",
        "total_steps = NUM_EPOCHS * len(train_dataloader)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=LEARNING_RATE,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1,  # 10% warmup\n",
        "    anneal_strategy='cos'\n",
        ")\n",
        "\n",
        "print(f\"‚úì Scheduler configured: OneCycleLR with {total_steps} total steps ({len(train_dataloader)} steps per epoch)\")\n",
        "print(\"Starting training...\")\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    # Training\n",
        "    train_metrics = train_one_epoch(\n",
        "        model=dinov2_vitb14_reg,\n",
        "        dataloader=train_dataloader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        scaler=None,  # Disabled mixed precision\n",
        "        epoch=epoch\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    val_loss = validate_model(\n",
        "        model=dinov2_vitb14_reg,\n",
        "        dataloader=val_dataloader,\n",
        "        criterion=criterion\n",
        "    )\n",
        "\n",
        "    # Log metrics\n",
        "    train_losses.append(train_metrics['total_loss'])\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {train_metrics['total_loss']:.4f}\")\n",
        "    print(f\"  - Correspondence: {train_metrics['correspondence_loss']:.4f}\")\n",
        "    print(f\"  - Consistency: {train_metrics['consistency_loss']:.4f}\")\n",
        "    print(f\"  - Smoothness: {train_metrics['smoothness_loss']:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': dinov2_vitb14_reg.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'train_metrics': train_metrics\n",
        "        }, 'best_dinov2_light_finetune.pth')\n",
        "        print(f\"üíæ New best model saved! (val_loss: {val_loss:.4f})\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to save the checkpoint in Google Drive\n",
        "gdrive_save_path = '/content/drive/MyDrive/DINOv3_checkpoints'\n",
        "checkpoint_filename = 'best_dinov2_light_finetune.pth'\n",
        "local_checkpoint_path = checkpoint_filename\n",
        "\n",
        "# Create the target directory in Google Drive if it doesn't exist\n",
        "os.makedirs(gdrive_save_path, exist_ok=True)\n",
        "\n",
        "# Check if the checkpoint file exists locally\n",
        "if os.path.exists(local_checkpoint_path):\n",
        "    # Copy the checkpoint to Google Drive\n",
        "    destination_path = os.path.join(gdrive_save_path, checkpoint_filename)\n",
        "    shutil.copy(local_checkpoint_path, destination_path)\n",
        "    print(f\"‚úÖ Checkpoint '{checkpoint_filename}' successfully uploaded to Google Drive at '{destination_path}'\")\n",
        "else:\n",
        "    print(f\"‚ùå Checkpoint file '{checkpoint_filename}' not found locally. Skipping upload.\")\n",
        "\n",
        "\n",
        "# Utility functions for when you're ready to train\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, loss, filepath):\n",
        "    \"\"\"\n",
        "    Save training checkpoint\n",
        "    \"\"\"\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'loss': loss,\n",
        "    }, filepath)\n",
        "\n",
        "def load_checkpoint(filepath, model, optimizer=None, scheduler=None):\n",
        "    \"\"\"\n",
        "    Load training checkpoint\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler is not None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    return checkpoint['epoch'], checkpoint['loss']\n",
        "\n",
        "def plot_training_curves(train_losses, val_losses):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss curves\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.title('Loss Curves')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    if len(train_losses) > 1:\n",
        "        plt.plot(np.gradient(train_losses), label='Train Loss Gradient')\n",
        "        plt.plot(np.gradient(val_losses), label='Val Loss Gradient')\n",
        "        plt.title('Loss Gradients (Learning Progress)')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss Gradient')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
